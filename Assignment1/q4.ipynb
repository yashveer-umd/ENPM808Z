{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `CNNModel` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `CNNModel` class defines a Convolutional Neural Network (CNN) architecture suitable for `IMAGE` classification tasks. It inherits from the `nn.Module` of PyTorch, providing the foundational structure to build, train, and evaluate deep learning models in PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "### Attributes:\n",
    "- **conv**: The convolutional layers of the CNN.\n",
    "- **fc**: A fully connected layer used for classification (not yet defined in the provided code).\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `__init__(self, args)`\n",
    "- **Purpose**: Initializes the CNN model.\n",
    "- **Parameters**: \n",
    "  - **args**: A set of arguments containing hyperparameters and configurations for the CNN.\n",
    "- **Description**: \n",
    "  - Constructs the convolutional layers using the provided arguments. The detailed architecture has to be filled in under the `TODO` comment.\n",
    "\n",
    "---\n",
    "\n",
    "#### `forward(self, x)`\n",
    "- **Purpose**: Defines the forward pass of the CNN.\n",
    "- **Parameters**: \n",
    "  - **x**: An input tensor with shape (batch_size, channels, height, width), typically representing a batch of images.\n",
    "- **Returns**: \n",
    "  - An output tensor (not yet defined in the code) with shape (batch_size, num_classes), representing the model's predictions for each image in the batch.\n",
    "- **Description**: \n",
    "  - Processes the input tensor through the CNN's layers to produce a prediction for each image. The exact forward pass operations need to be defined under the `TODO` comment.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This class provides a template for a CNN model suitable for MNIST classification. Several components, including the exact architecture of the convolutional layers and the forward pass operations, are indicated with `TODO` comments, suggesting that these parts need further implementation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL NEEDS TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------# \n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "\n",
    "\"\"\"\n",
    "define modules of model\n",
    "\"\"\"\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network (CNN) image classification.\n",
    "    \n",
    "    Attributes:\n",
    "        conv: Convolutional layers.\n",
    "        fc: Fully connected layer for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize the CNN model with given arguments.\n",
    "        \n",
    "        Args:\n",
    "            args: Arguments containing hyperparameters.\n",
    "        \"\"\"\n",
    "        # TODO:\n",
    "        # - Define the architecture for the convolutional layers using nn.Sequential.\n",
    "        # - Utilize the hyperparameters given by the `args` argument (like the number of channels, kernel size, etc.).\n",
    "        # - Add the necessary convolutional layers (`nn.Conv2d`), activation functions (`nn.ReLU`, etc.), and pooling layers (`nn.MaxPool2d`, etc.) as needed.\n",
    "        # - Ensure the depth and the sizes of the feature maps after each layer align with the desired architecture.\n",
    "        # Convolutional Layers\n",
    "        self.conv = nn.Sequential(\n",
    "            # use the arguments to build your CNN Model Here\n",
    "            )\n",
    "        \n",
    "        # TODO:\n",
    "        # - Define the fully connected (dense) layers for the network.\n",
    "        # - Determine the input dimension to the first fully connected layer. This should be the flattened size of the feature map produced by the last convolutional layer.\n",
    "        # - Define linear layers (`nn.Linear`) based on the desired number of neurons in the hidden layers and the number of output classes.\n",
    "        # - Remember to add activation functions (`nn.ReLU`, etc.) in between these linear layers.\n",
    "        # - Optionally, consider adding dropout layers (`nn.Dropout`) for regularization if needed.\n",
    "        # Fully Connected Layers\n",
    "        self.fc = None\n",
    "\n",
    "    # Feed features to the model\n",
    "    def forward(self, x):  # default\n",
    "        \"\"\"\n",
    "        Forward pass of the CNN.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, channels, height, width)\n",
    "            \n",
    "        Returns:\n",
    "            result: Output tensor of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # TODO:\n",
    "        # - Pass the input tensor `x` through the convolutional layers defined in `self.conv`.\n",
    "        # - Flatten the resulting feature map to make it suitable for the fully connected layers.\n",
    "        # - Pass the flattened tensor through the fully connected layers (`self.fc`).\n",
    "        # - Ensure the final output tensor has a shape compatible with the expected number of classes for the classification task.\n",
    "        # - Consider using an activation function like softmax if needed at the output (especially if the loss function you're planning to use requires it).\n",
    "\n",
    "        x_out = None        \n",
    "        return x_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yashv\\miniconda3\\envs\\practice1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "data_path: ../New_Code/data/\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../New_Code/data/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 195\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    194\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 195\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60.0\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m mins\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 176\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    173\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_mps \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 176\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../New_Code/data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m model \u001b[38;5;241m=\u001b[39m CNNModel(args\u001b[38;5;241m=\u001b[39margs)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    180\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlearning_rate)\n",
      "Cell \u001b[1;32mIn[2], line 40\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(DATA_PATH, batch_size)\u001b[0m\n\u001b[0;32m     34\u001b[0m test_trans \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     35\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     36\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,), (\u001b[38;5;241m0.5\u001b[39m,))\n\u001b[0;32m     37\u001b[0m ])\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Create train and test datasets\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATA_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_trans\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m ImageFolder(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtest_trans)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Create data loaders\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yashv\\miniconda3\\envs\\practice1\\lib\\site-packages\\torchvision\\datasets\\folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ):\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32mc:\\Users\\yashv\\miniconda3\\envs\\practice1\\lib\\site-packages\\torchvision\\datasets\\folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32mc:\\Users\\yashv\\miniconda3\\envs\\practice1\\lib\\site-packages\\torchvision\\datasets\\folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yashv\\miniconda3\\envs\\practice1\\lib\\site-packages\\torchvision\\datasets\\folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../New_Code/data/train'"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL CAN TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------# \n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import time\n",
    "import h5py\n",
    "import argparse\n",
    "import os.path\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from torchsummary import summary\n",
    "# from utils import str2bool  # Utility function for argument parsing\n",
    "\n",
    "\n",
    "def load_data(DATA_PATH, batch_size):\n",
    "    print(f\"data_path: {DATA_PATH}\")\n",
    "\n",
    "    # Define transformations\n",
    "    train_trans = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    test_trans = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    # Create train and test datasets\n",
    "    train_dataset = ImageFolder(root=f\"{DATA_PATH}train\", transform=train_trans)\n",
    "    test_dataset = ImageFolder(root=f\"{DATA_PATH}test\", transform=test_trans)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def compute_accuracy(y_pred, y_batch):\n",
    "    accy = (y_pred == y_batch).sum().item() / len(y_batch)\n",
    "    return accy\n",
    "\n",
    "def validate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_labels in val_loader:\n",
    "            x_batch, y_labels = x_batch.to(device), y_labels.to(device)\n",
    "            output_y = model(x_batch)\n",
    "            loss = nn.CrossEntropyLoss()(output_y, y_labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(output_y, 1)\n",
    "            val_accuracy += (preds == y_labels).float().mean()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_labels.cpu().numpy())\n",
    "\n",
    "    confusion = confusion_matrix(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return val_loss/len(val_loader), val_accuracy/len(val_loader), confusion, precision, recall, f1\n",
    "\n",
    "def print_model_size(model):\n",
    "    summary(model, (1, 28, 28)) \n",
    "\n",
    "def adjust_learning_rate(learning_rate, optimizer, epoch, decay):\n",
    "    lr = learning_rate\n",
    "    if epoch > 5:\n",
    "        lr = 0.001\n",
    "    if epoch >= 10:\n",
    "        lr = 0.0001\n",
    "    if epoch > 20:\n",
    "        lr = 0.00001\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "\n",
    "def train_one_epoch(model, optimizer, train_loader, device):\n",
    "    model.train()\n",
    "    for batch_id, (x_batch, y_labels) in tqdm(enumerate(train_loader), desc=\"Training\", leave=False):  \n",
    "        x_batch, y_labels = Variable(x_batch).to(device), Variable(y_labels).to(device)\n",
    "\n",
    "        \n",
    "\n",
    "        output_y = model(x_batch)\n",
    "        loss = nn.CrossEntropyLoss()(output_y, y_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, y_pred = torch.max(output_y.data, 1)\n",
    "        accy = compute_accuracy(y_pred, y_labels)\n",
    "\n",
    "        # Here, you can add code to log or print the loss and accuracy if you want\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_accy = 0\n",
    "    for batch_id, (x_batch, y_labels) in tqdm(enumerate(test_loader), desc=\"Testing\", leave=False):  \n",
    "        x_batch, y_labels = Variable(x_batch).to(device), Variable(y_labels).to(device)\n",
    "        output_y = model(x_batch)\n",
    "        _, y_pred = torch.max(output_y.data, 1)\n",
    "        accy = compute_accuracy(y_pred, y_labels)\n",
    "        total_accy += accy\n",
    "    return total_accy / len(test_loader)\n",
    "    \n",
    "\n",
    "def main():\n",
    "\n",
    "    \n",
    "    # TODO These args are for use in main and also for building your network. The current values are defaults and can be edited to suite your needs!\n",
    "    \"\"\"\n",
    "    args:\n",
    "    \"-mode\", dest=\"mode\", type=str, default='train', help=\"train or test\"\n",
    "    \"-num_epochs\", dest=\"num_epoches\", type=int, default=40, help=\"num of epoches\"\n",
    "    \"-fc_hidden1\", dest=\"fc_hidden1\", type=int, default=100, help=\"dim of hidden neurons\"\n",
    "    \"-fc_hidden2\", dest=\"fc_hidden2\", type=int, default=100, help=\"dim of hidden neurons\"\n",
    "    \"-learning_rate\", dest =\"learning_rate\", type=float, default=0.001, help = \"learning rate\"\n",
    "    \"-decay\", dest =\"decay\", type=float, default=0.5, help = \"learning rate\"\n",
    "    \"-batch_size\", dest=\"batch_size\", type=int, default=100, help=\"batch size\"\n",
    "    \"-dropout\", dest =\"dropout\", type=float, default=0.4, help = \"dropout prob\"\n",
    "    \"-rotation\", dest=\"rotation\", type=int, default=10, help=\"image rotation\"\n",
    "    \"-load_checkpoint\", dest=\"load_checkpoint\", type=bool, default=True, help=\"true of false\"\n",
    "\n",
    "    \"-activation\", dest=\"activation\", type=str, default='relu', help=\"activation function\"\n",
    "    \"-channel_out1\", dest='channel_out1', type=int, default=64, help=\"number of channels\"\n",
    "    \"-channel_out2\", dest='channel_out2', type=int, default=64, help=\"number of channels\"\n",
    "    \"-k_size\", dest='k_size', type=int, default=4, help=\"size of filter\"\n",
    "    \"-pooling_size\", dest='pooling_size', type=int, default=2, help=\"size for max pooling\"\n",
    "    \"-stride\", dest='stride', type=int, default=1, help=\"stride for filter\"\n",
    "    \"-max_stride\", dest='max_stride', type=int, default=2, help=\"stride for max pooling\"\n",
    "    \"-ckp_path\", dest='ckp_path', type=str, default=\"checkpoint\", help=\"path of checkpoint\"\n",
    "    \"\"\"\n",
    "    args = argparse.Namespace(\n",
    "        mode='train',\n",
    "        num_epochs=3,\n",
    "        fc_hidden1=100,\n",
    "        fc_hidden2=100,\n",
    "        learning_rate=0.002,\n",
    "        decay=0.5,\n",
    "        batch_size=100,\n",
    "        dropout=0.4,\n",
    "        rotation=10,\n",
    "        load_checkpoint=False,\n",
    "        activation='relu',\n",
    "        channel_out1=64,\n",
    "        channel_out2=64,\n",
    "        stride=1,\n",
    "        max_stride=2,\n",
    "        ckp_path='checkpoint',\n",
    "        k_size=4,\n",
    "        pooling_size=2,\n",
    "        )\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    use_mps = torch.backends.mps.is_available()\n",
    "    device = torch.device(\"mps\" if use_mps else \"cpu\")\n",
    "    print(f\"device: {device}\")\n",
    "\n",
    "    train_loader, test_loader = load_data(\"data/sampled_CINIC_10/sampled_CINIC_10/\", args.batch_size)\n",
    "\n",
    "    model = CNNModel(args=args).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        adjust_learning_rate(args.learning_rate, optimizer, epoch, args.decay)\n",
    "        train_one_epoch(model, optimizer, train_loader, device)\n",
    "        test_accuracy = test_model(model, test_loader, device)\n",
    "        print(f\"Epoch {epoch+1}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "        # Optionally, save model checkpoint here\n",
    "\n",
    "    print(\"Training Complete!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    end_time = time.time()\n",
    "    print(f\"Running time: {(end_time - start_time) / 60.0:.2f} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
