{
 "cells": [
  {
   "attachments": {
    "3ecnl.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABRE0lEQVR4nO3deXhMZ/8G8HsySWayJ4gsEhIaJAQlaKS2NqRetKqIpQ2haEnRWNq0JEItpRQVFCW6aNXeF1URSy0pShU/QZGI0sSekXXGzPn9cd4M0yRkmczEcX+uay6ZZ87yPc9st3Oec0YmCIIAIiIiIomwMHcBRERERMbEcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ0RERJLCcENERESSwnBDREREksJwQ2QiPj4+GDJkiNnWP2TIEPj4+Bi05eTk4O2334a7uztkMhnGjRuH9PR0yGQyJCYmmrzGTp06oVOnTiZfb3Xw722viufB3K9BIlNhuCGqpEuXLmHkyJGoX78+lEolHB0dERISgoULFyI/P9/c5T3WzJkzkZiYiHfffRfffPMN3nrrrSpf59mzZzF16lSkp6dX+brKat++fZDJZPqblZUV6tevj4iICFy+fNnc5ZXL4cOHMXXqVNy7d8/cpRCZjaW5CyB6mm3fvh19+/aFQqFAREQEmjZtCrVajYMHD2LixIn4v//7PyxfvtzcZQIAVqxYAZ1OZ9C2Z88evPDCC4iLi9O3CYKA/Px8WFlZVUkdZ8+eRXx8PDp16lRsT9KuXbuqZJ1lNWbMGLRu3RoajQYnTpzA8uXLsX37dpw+fRqenp4mraVevXoVeh4OHz6M+Ph4DBkyBM7OzgaPnT9/HhYW/D8tSR/DDVEFpaWloX///qhXrx727NkDDw8P/WOjR4/GxYsXsX37djNWaKikL8kbN24gICDAoE0mk0GpVJqqLAPW1tZmWW+R9u3bo0+fPgCAyMhINGzYEGPGjMGaNWsQExNT4jy5ubmws7Mzei1V8TwoFAqjLo+oumKEJ6qgOXPmICcnB1999ZVBsCny3HPPYezYsaXOf+fOHUyYMAGBgYGwt7eHo6MjunXrhj///LPYtF988QWaNGkCW1tbuLi4ICgoCGvXrtU/fv/+fYwbNw4+Pj5QKBSoXbs2unTpghMnTuineXTMTdFhmLS0NGzfvl1/OCY9Pb3UsR7nzp1Dv3794OrqChsbGzRq1Agff/yx/vErV65g1KhRaNSoEWxsbFCzZk307dvX4PBTYmIi+vbtCwDo3Lmzfr379u0DUPKYmxs3bmDYsGFwc3ODUqlE8+bNsWbNGoNpimr+7LPPsHz5cjRo0AAKhQKtW7fGsWPHSn0OnuSll14CIAZZAJg6dSpkMhnOnj2LgQMHwsXFBS+++KJ++m+//RatWrWCjY0NatSogf79++Pq1avFlltUo42NDdq0aYMDBw4Um6Yiz8PUqVMxceJEAICvr6/B8wqUPObm8uXL6Nu3L2rUqAFbW1u88MILxUJ50evlxx9/xIwZM+Dl5QWlUomXX34ZFy9eNJj2r7/+whtvvAF3d3colUp4eXmhf//+yM7OfkJvExkP99wQVdB///tf1K9fH+3atavQ/JcvX8aWLVvQt29f+Pr6IisrC19++SU6duyIs2fP6g+DrFixAmPGjEGfPn0wduxYFBQU4NSpUzhy5AgGDhwIAHjnnXewYcMGREVFISAgALdv38bBgweRmpqKli1bFlu3v78/vvnmG7z//vvw8vLC+PHjAQCurq64efNmselPnTqF9u3bw8rKCiNGjICPjw8uXbqE//73v5gxYwYA4NixYzh8+DD69+8PLy8vpKenY+nSpejUqRPOnj0LW1tbdOjQAWPGjMGiRYvw0Ucfwd/fX19PSfLz89GpUydcvHgRUVFR8PX1xfr16zFkyBDcu3evWHhcu3Yt7t+/j5EjR0Imk2HOnDno3bs3Ll++XKHDbJcuXQIA1KxZ06C9b9++8PPzw8yZMyEIAgBgxowZmDJlCvr164e3334bN2/exBdffIEOHTrgjz/+0B8i+uqrrzBy5Ei0a9cO48aNw+XLl/Hqq6+iRo0a8Pb2fmw9T3oeevfujQsXLuD777/H559/jlq1agEQn9eSZGVloV27dsjLy8OYMWNQs2ZNrFmzBq+++io2bNiA119/3WD62bNnw8LCAhMmTEB2djbmzJmDQYMG4ciRIwAAtVqNsLAwFBYW4r333oO7uzuuXbuGbdu24d69e3BycirfE0BUUQIRlVt2drYAQHjttdfKPE+9evWEwYMH6+8XFBQIWq3WYJq0tDRBoVAI06ZN07e99tprQpMmTR67bCcnJ2H06NGPnWbw4MFCvXr1itXUvXv3YjUAEFavXq1v69Chg+Dg4CBcuXLFYFqdTqf/Oy8vr9g6U1JSBADC119/rW9bv369AEDYu3dvsek7duwodOzYUX9/wYIFAgDh22+/1bep1WohODhYsLe3F1QqlUHNNWvWFO7cuaOfduvWrQIA4b///W/xDnnE3r17BQDCqlWrhJs3bwrXr18Xtm/fLvj4+AgymUw4duyYIAiCEBcXJwAQBgwYYDB/enq6IJfLhRkzZhi0nz59WrC0tNS3q9VqoXbt2kKLFi2EwsJC/XTLly8XABhse0Wfh7lz5woAhLS0tGLb+e/X4Lhx4wQAwoEDB/Rt9+/fF3x9fQUfHx/967Oof/z9/Q3qXrhwoQBAOH36tCAIgvDHH38IAIT169cXWzeRKfGwFFEFqFQqAICDg0OFl6FQKPSDO7VaLW7fvg17e3s0atTI4HCSs7Mz/v7778ceXnF2dsaRI0dw/fr1CtdTmps3b+LXX3/F0KFDUbduXYPHZDKZ/m8bGxv93xqNBrdv38Zzzz0HZ2dng+0pjx07dsDd3R0DBgzQt1lZWWHMmDHIycnB/v37DaYPDw+Hi4uL/n779u0BoMxnPA0dOhSurq7w9PRE9+7dkZubizVr1iAoKMhgunfeecfg/qZNm6DT6dCvXz/cunVLf3N3d4efnx/27t0LAPj9999x48YNvPPOOwbji4YMGfLEvRplfR7KY8eOHWjTpo3BoTV7e3uMGDEC6enpOHv2rMH0kZGRBnX/u3+LtuGXX35BXl5ehWoiMgaGG6IKcHR0BCCOdakonU6Hzz//HH5+flAoFKhVqxZcXV1x6tQpg/EJH3zwAezt7dGmTRv4+flh9OjROHTokMGy5syZgzNnzsDb2xtt2rTB1KlTjXYKc9FymjZt+tjp8vPzERsbC29vb4PtuXfvXoXHW1y5cgV+fn7FzvApOox15coVg/Z/f+kXBZ27d++WaX2xsbFISkrCnj17cOrUKVy/fr3E0+N9fX0N7v/1118QBAF+fn5wdXU1uKWmpuLGjRsG9fr5+RnMX3Tq+eOU9XkojytXrqBRo0bF2ivav76+voiOjsbKlStRq1YthIWFISEhgeNtyOQ45oaoAhwdHeHp6YkzZ85UeBkzZ87ElClTMHToUEyfPh01atSAhYUFxo0bZ3DKtr+/P86fP49t27Zh586d2LhxI5YsWYLY2FjEx8cDAPr164f27dtj8+bN2LVrF+bOnYtPP/0UmzZtQrdu3Sq9vWXx3nvvYfXq1Rg3bhyCg4Ph5OQEmUyG/v37FzsFvarI5fIS24X/jYt5ksDAQISGhj5xukf3UgFiUJXJZPj5559LrMHe3r5M66/uytK/8+bNw5AhQ7B161bs2rULY8aMwaxZs/Dbb7/By8vLVKXSM47hhqiCevTogeXLlyMlJQXBwcHlnn/Dhg3o3LkzvvrqK4P2e/fu6QeCFrGzs0N4eDjCw8OhVqvRu3dvzJgxAzExMfrThT08PDBq1CiMGjUKN27cQMuWLTFjxoxKh5uiPQpPCnIbNmzA4MGDMW/ePH1bQUFBsYvJlecQSr169XDq1CnodDqDvTfnzp3TP14dNGjQAIIgwNfXFw0bNix1uqJ6//rrL/2ZWIB4GC8tLQ3Nmzcvdd6yPg/l7d/z588Xa69s/wYGBiIwMBCTJ0/G4cOHERISgmXLluGTTz6p0PKIyouHpYgqaNKkSbCzs8Pbb7+NrKysYo9funQJCxcuLHV+uVxebI/C+vXrce3aNYO227dvG9y3trZGQEAABEGARqOBVqstttu/du3a8PT0RGFhYXk3qxhXV1d06NABq1atQkZGhsFjj9Zf0vZ88cUX0Gq1Bm1F14QpyxV0//Of/yAzMxPr1q3Ttz148ABffPEF7O3t0bFjx/JuTpXo3bs35HI54uPji/WBIAj65zAoKAiurq5YtmwZ1Gq1fprExMQn9kdZn4fy9u/Ro0eRkpKib8vNzcXy5cvh4+NT7BpIT6JSqfDgwQODtsDAQFhYWBjltUhUVtxzQ1RBDRo0wNq1axEeHg5/f3+DKxQfPnxYf8pyaXr06IFp06YhMjIS7dq1w+nTp/Hdd98VG3vRtWtXuLu7IyQkBG5ubkhNTcXixYvRvXt3ODg44N69e/Dy8kKfPn3QvHlz2NvbY/fu3Th27JjBXpTKWLRoEV588UW0bNkSI0aMgK+vL9LT07F9+3acPHlSvz3ffPMNnJycEBAQgJSUFOzevbvYadQtWrSAXC7Hp59+iuzsbCgUCrz00kuoXbt2sfWOGDECX375JYYMGYLjx4/Dx8cHGzZswKFDh7BgwYJKDeg2pgYNGuCTTz5BTEwM0tPT0atXLzg4OCAtLQ2bN2/GiBEjMGHCBFhZWeGTTz7ByJEj8dJLLyE8PBxpaWlYvXr1E8fcAGV7Hlq1agUA+Pjjj9G/f39YWVmhZ8+eJV5o8MMPP8T333+Pbt26YcyYMahRowbWrFmDtLQ0bNy4sdxXM96zZw+ioqLQt29fNGzYEA8ePMA333wDuVyON954o1zLIqoUM52lRSQZFy5cEIYPHy74+PgI1tbWgoODgxASEiJ88cUXQkFBgX66kk4FHz9+vODh4SHY2NgIISEhQkpKSrHTob/88kuhQ4cOQs2aNQWFQiE0aNBAmDhxopCdnS0IgiAUFhYKEydOFJo3by44ODgIdnZ2QvPmzYUlS5YY1FmZU8EFQRDOnDkjvP7664Kzs7OgVCqFRo0aCVOmTNE/fvfuXSEyMlKoVauWYG9vL4SFhQnnzp0rtt2CIAgrVqwQ6tevL8jlcoPTwv+97YIgCFlZWfrlWltbC4GBgcVqK6p57ty5wr8BEOLi4oq1P6roVOcnncJcdCr4zZs3S3x848aNwosvvijY2dkJdnZ2QuPGjYXRo0cL58+fN5huyZIlgq+vr6BQKISgoCDh119/LbbtFX0eBEEQpk+fLtSpU0ewsLAwOC28pOfi0qVLQp8+ffTLa9OmjbBt27Yy9c+/a7x8+bIwdOhQoUGDBoJSqRRq1KghdO7cWdi9e/djepXI+GSCUMaRdkRERERPAY65ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSXnmLuKn0+lw/fp1ODg4VPiXdImIiMi0BEHA/fv34enp+cQLTD5z4eb69evw9vY2dxlERERUAVevXn3ij7A+c+Gm6HLtV69ehaOjo1GXrdFosGvXLnTt2hVWVlZGXTY9xH42DfazabCfTYd9bRpV1c8qlQre3t5l+tmVZy7cFB2KcnR0rJJwY2trC0dHR75xqhD72TTYz6bBfjYd9rVpVHU/l2VICQcUExERkaQw3BAREZGkMNwQERGRpDDcEBERkaQw3BAREZGkMNwQERGRpDDcEBERkaQw3BAREZGkMNwQERGRpDxzVyiubnQ6IDUV+OQT4NAh4PZtQKsFrK0BT0/Azw/QaIC0NEClAiwtARsb8d/8fCAnB7CwAKysgMJCcV6tFhAEQC4H7O0BtVqcFxCXa28vTi8IQEEBoFQCCgVw/744j4sLkJsr3rRasUa1GnByAmrVAmQyIDNTXJ8gAHZ24t85OeJ9W1txOktLcf6CAvFvZ2dxPZmZ4rry88Vt0+nExy0txbqK6lSrxW3TasV1FBQADx4ADg5AQgLQrJm4XQUF4vofPBCnr1MHqFtXXM6dO+I2ZWYCd++Ky7K1NexDCwtxGk9PcZszM8WatNqHz5G1rgCeugw0kZ2Fr+IqbNyc8PKYQAQPbABLSwCXLgFnzgDZ2WIBTZqIRSiVpn5JERWXmQmcPi2+STIyxA+Uxo3FN/PvvwN//QUEBT38UMjPf/iGe/AA8PEBAgLEN/DBg4CXl/jazsgQ3yiOjuKHFwDcuye+kWvWFD8Ijh0T32Q1aojzuLlBF9gcVy/mY/9Xf+HovYY4LzRGLaUKt+188LfGHTk54meAq6v475074uKLPit0OvG9CoirDgkRSz5/Xly9o6P4OVY0vyCIb83bt4Fp08T2ggLxMRsbIC/v4WeDpaV439ZWXIaFhTidn5/Ylp8vbvbNm+JnHiBOUzRtXp64bIVC/HxTKsXPO5ns4Wdt0efrgwfifW9v4LnnxLqvXRM/SrKyHn6mWViI8xdtu7U10LYt8MMPYrdSCQQz2r9/v9CjRw/Bw8NDACBs3rz5ifPs3btXeP755wVra2uhQYMGwurVq8u1zuzsbAGAkJ2dXbGiH0OtVgtbtmwR1Gp1maY/e1YQAgMFQXzr8VbWm42N2M82NmqTrE+BfGEcPhPS4SFoAEELCA8A4QZchATFOOHiq+MEwdnZcCYPD0H47DNByM83+uvMVMr7eqaKqfJ+/ucfQXj+eUGwtBQEmczsb+Ci90/Re0kLCFlwEW7DUUhBa8EV/0jms8MUt1atquZlUxlV9Zouz/e3WQ9L5ebmonnz5khISCjT9GlpaejevTs6d+6MkydPYty4cXj77bfxyy+/VHGlxpeaCvToIf5niqo3a6hRC7fggDxYAJBBPJ5bE3cxpHAJPH5KgHDvnuFMeXnArVvi7icic7p7V9xtUbRL14wEPHz/yP/3twxADdyFAmo4QAUX3DVniU+d48fFnW5kyKyHpbp164Zu3bqVefply5bB19cX8+bNAwD4+/vj4MGD+PzzzxEWFlZVZRqdTifuTrx82dyVUFnchyPm4EPIIOBdLIUTVPoPZRs8DC9FH9xwcgLeeQf48ENxPzOROfn7A1u3Aq+9Jh6OMlPAKW2tOgCFUCId9dAXG3AB/qYsSxKOHxcP3fEQ1UNP1ZiblJQUhIaGGrSFhYVh3Lhxpc5TWFiIwsJC/X3V/wafaDQaaDQao9ZXtLwnLTcjA/jpJ/FYL5WfjY3G4F9T0MAWCzERFpBjGFbCEfdR0u/SyhwdgWHDgOho8QC9kV9jplTW1zNVjkn6uVEjYPNmoH9/4MoVswScktZYFGwy4I0IfIN0NIINqq4fzPHZYSqRkcCGDeauQlRVr+nyLE8mCGbeT/k/MpkMmzdvRq9evUqdpmHDhoiMjERMTIy+bceOHejevTvy8vJgU0JamDp1KuLj44u1r127Fra2tkapnYiIiKpWXl4eBg4ciOzsbDg+Ya/4U7XnpiJiYmIQHR2tv69SqeDt7Y2uXbs+sXPKS6PRICkpCV26dIFV0Wk/JcjIAPr0EUf2U/nZ2GiwalUShg7tgvz80vvZ2BygwvuYX/Y9N0/5Iamyvp6pckzWz6mp1X7PzfkqPiRlrs8OU+jSpXrtuamK13TRkZeyeKrCjbu7O7KysgzasrKy4OjoWOJeGwBQKBRQKBTF2q2srKrsg+RJy/b1BV59VTwlkSouP9/KZB9QjsjGWMzFyEfG3JRElp8vnqeu1QIxMeL4m6dcVb5X6KEq7eczZ4DXX6+WY26skI/6yMe36Ie+2IBUNK3yWkz52WEqq1c/vJRGdWHs13R5lvVUXcQvODgYycnJBm1JSUkIDg42U0UVY2Eh/geqfn1zV0Jl4QAVJmE23sEyg2AjAMiHNfIhvuH0gSc7G1i2DJg9++EFhojMJTXV7IOJAZT6HwILAAoUwAdXsB590BCppixLElq14mDifzNruMnJycHJkydx8uRJAOKp3idPnkRGRgYA8ZBSRESEfvp33nkHly9fxqRJk3Du3DksWbIEP/74I95//31zlF8p/v7Atm1AYKC5K6EnUcMat1AL92ELHcRQowNwGy5IVIzCP6+OhszZ2XCmoisZWlubvmCiR7m4iHsQ5XLxSnBmJMPD94/2f38LAO7ABYWwxn044i5czFniU6dVK/E6jGTIrIelfv/9d3Tu3Fl/v2hszODBg5GYmIh//vlHH3QAwNfXF9u3b8f777+PhQsXwsvLCytXrnyqTgN/lL8/cPIkr1BckSsUA0C9eqa6QrESibrR2KXrWewKxSP0VyiO4BWKqXpydwd27Kg2Vyi2+N8Viv8u5QrFtTTuUFbhFYoBsa3oSr+8QrE0mTXcdOrUCY87WSsxMbHEef74448qrMq0LCzE78Hvvzd3JU8PjUb8rD51ypTHmJUAGv7vVoLnnxdvRNWRu7t4K8ngweVbVt++lS7HAkA9ABHjgIgnTGtMRZ8dN29Wv/EpZFxP1ZgbIiIioidhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJYbghIiIiSWG4ISIiIklhuCEiIiJJMXu4SUhIgI+PD5RKJdq2bYujR48+dvoFCxagUaNGsLGxgbe3N95//30UFBSYqFoiIiKq7swabtatW4fo6GjExcXhxIkTaN68OcLCwnDjxo0Sp1+7di0+/PBDxMXFITU1FV999RXWrVuHjz76yMSVExERUXVl1nAzf/58DB8+HJGRkQgICMCyZctga2uLVatWlTj94cOHERISgoEDB8LHxwddu3bFgAEDnri3h4iIiJ4dluZasVqtxvHjxxETE6Nvs7CwQGhoKFJSUkqcp127dvj2229x9OhRtGnTBpcvX8aOHTvw1ltvlbqewsJCFBYW6u+rVCoAgEajgUajMdLWQL/MR/+lqsF+Ng32s2mwn02HfW0aVdXP5Vme2cLNrVu3oNVq4ebmZtDu5uaGc+fOlTjPwIEDcevWLbz44osQBAEPHjzAO++889jDUrNmzUJ8fHyx9l27dsHW1rZyG1GKpKSkKlkuGWI/mwb72TTYz6bDvjYNY/dzXl5emac1W7ipiH379mHmzJlYsmQJ2rZti4sXL2Ls2LGYPn06pkyZUuI8MTExiI6O1t9XqVTw9vZG165d4ejoaNT6NBoNkpKS0KVLF1hZWRl12fQQ+9k02M+mwX42Hfa1aVRVPxcdeSkLs4WbWrVqQS6XIysry6A9KysL7u7uJc4zZcoUvPXWW3j77bcBAIGBgcjNzcWIESPw8ccfw8Ki+BAihUIBhUJRrN3KyqrKXtxVuWx6iP1sGuxn02A/mw772jSM3c/lWZbZBhRbW1ujVatWSE5O1rfpdDokJycjODi4xHny8vKKBRi5XA4AEASh6oolIiKip4ZZD0tFR0dj8ODBCAoKQps2bbBgwQLk5uYiMjISABAREYE6depg1qxZAICePXti/vz5eP755/WHpaZMmYKePXvqQw4RERE928wabsLDw3Hz5k3ExsYiMzMTLVq0wM6dO/WDjDMyMgz21EyePBkymQyTJ0/GtWvX4Orqip49e2LGjBnm2gQiIiKqZsw+oDgqKgpRUVElPrZv3z6D+5aWloiLi0NcXJwJKiMiIqKnkdl/foGIiIjImBhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFIYboiIiEhSGG6IiIhIUhhuiIiISFLMHm4SEhLg4+MDpVKJtm3b4ujRo4+d/t69exg9ejQ8PDygUCjQsGFD7Nixw0TVEhERUXVnac6Vr1u3DtHR0Vi2bBnatm2LBQsWICwsDOfPn0ft2rWLTa9Wq9GlSxfUrl0bGzZsQJ06dXDlyhU4OzubvngiIiKqlswabubPn4/hw4cjMjISALBs2TJs374dq1atwocfflhs+lWrVuHOnTs4fPgwrKysAAA+Pj6mLJmIiIiqObOFG7VajePHjyMmJkbfZmFhgdDQUKSkpJQ4z08//YTg4GCMHj0aW7duhaurKwYOHIgPPvgAcrm8xHkKCwtRWFiov69SqQAAGo0GGo3GiFsE/fKMvVwyxH42DfazabCfTYd9bRpV1c/lWZ7Zws2tW7eg1Wrh5uZm0O7m5oZz586VOM/ly5exZ88eDBo0CDt27MDFixcxatQoaDQaxMXFlTjPrFmzEB8fX6x9165dsLW1rfyGlCApKalKlkuG2M+mwX42Dfaz6bCvTcPY/ZyXl1fmac16WKq8dDodateujeXLl0Mul6NVq1a4du0a5s6dW2q4iYmJQXR0tP6+SqWCt7c3unbtCkdHR6PWp9FokJSUhC5duugPm5HxsZ9Ng/1sGuxn02Ffm0ZV9XPRkZeyMFu4qVWrFuRyObKysgzas7Ky4O7uXuI8Hh4esLKyMjgE5e/vj8zMTKjValhbWxebR6FQQKFQFGu3srKqshd3VS6bHmI/mwb72TTYz6bDvjYNY/dzeZZltlPBra2t0apVKyQnJ+vbdDodkpOTERwcXOI8ISEhuHjxInQ6nb7twoUL8PDwKDHYEBER0bPHrNe5iY6OxooVK7BmzRqkpqbi3XffRW5urv7sqYiICIMBx++++y7u3LmDsWPH4sKFC9i+fTtmzpyJ0aNHm2sTiIiIqJox65ib8PBw3Lx5E7GxscjMzESLFi2wc+dO/SDjjIwMWFg8zF/e3t745Zdf8P7776NZs2aoU6cOxo4diw8++MBcm0BERETVjNkHFEdFRSEqKqrEx/bt21esLTg4GL/99lsVV0VERERPK7P//AIRERGRMTHcEBERkaQw3BAREZGkmH3MDRERUVXRarUGPwdgaWmJgoICaLVaM1cmXZXpZ2tra4MTiSqK4YaIiCRHEARkZmbi3r17Bm3u7u64evUqZDKZ+YqTuMr0s4WFBXx9fSt97TqGGyIikpyiYFO7dm3Y2tpCJpNBp9MhJycH9vb2Rtk7QCWraD/rdDpcv34d//zzD+rWrVupAMpwQ0REkqLVavXBpmbNmvp2nU4HtVoNpVLJcFOFKtPPrq6uuH79Oh48eFCpn27gs0tERJJSNMbG1tbWzJVQeRUdjqrsmCiGGyIikiSOq3n6GOs5Y7ghIiIiSWG4ISIiIkmpULjZuXMnDh48qL+fkJCAFi1aYODAgbh7967RiiMiInrWZGZm4r333kP9+vWhUCjg7e2Nnj17Ijk52dylFZOYmAhnZ2dzl1FMhcLNxIkToVKpAACnT5/G+PHj8Z///AdpaWmIjo42aoFERETmoNMB6enA6dPivzpd1a8zPT0drVq1wp49ezB37lycPn0aO3fuROfOnTF69OgKLVOtVpfYXjTwWooqFG7S0tIQEBAAANi4cSN69OiBmTNnIiEhAT///LNRCyQiIjK11FRg9mwgNhaYPl38d/Zssb0qjRo1CjKZDEePHsUbb7yBhg0bokmTJoiOjsZvv/0GAMjIyMBrr70Ge3t7ODo6ol+/fsjKytIvY+rUqWjRogVWrlwJX19fKJVKAOJg3aVLl+LVV1+FnZ0dZsyYAQDYunUrWrZsCaVSifr16yM+Ph4PHjzQL+/evXsYOXIk3NzcoFQq0bRpU2zbtg379u1DZGQksrOzIZPJIJPJMHXq1KrtoDKq0HVurK2tkZeXBwDYvXs3IiIiAAA1atTQ79EhIiJ6GqWmAosWAbduAd7egJ0dkJsL/PEHcPUqMGYM4O9v/PXeuXMHO3fuxIwZM2BnZ1fscWdnZ+h0On2w2b9/Px48eIDRo0cjPDwc+/bt00978eJFbNy4EZs2bYJcLte3T506FbNnz8aCBQtgaWmJAwcOICIiAosWLUL79u1x6dIljBgxAgAQFxcHnU6Hbt264f79+/j222/RoEEDnD17FnK5HO3atcOCBQsQGxuL8+fPAwDs7e2N3zEVUKFw8+KLLyI6OhohISE4evQo1q1bBwC4cOECvLy8jFogERGRqeh0wObNYrAJCACKzkx2dBTvnz0LbNkCNGoEGPs6gBcvXoQgCGjcuHGp0yQnJ+P06dNIS0uDt7c3AODrr79GkyZNcOzYMbRu3RqAeCjq66+/hqurq8H8AwcORGRkpP7+0KFD8eGHH2Lw4MEAgPr162P69OmYNGkS4uLisHv3bhw9ehSpqalo2LChfpoiTk5OkMlkcHd317fpTHH87gkq9NQsXrwYlpaW2LBhA5YuXYo6deoAAH7++We88sorRi2QiIjIVDIygHPnxD02/77kikwGeHmJe3YyMoy/bkEQnjhNamoqvL299cEGAAICAuDs7IzUR46Z1atXr1iwAYCgoCCD+3/++SemTZsGe3t7/W348OH4559/kJeXh5MnT8LLy0sfbJ4WFdpzU7duXWzbtq1Y++eff17pgoiIiMzl/n2goEA8FFUSOzvg2jVxOmPz8/ODTCbDuXPnKr2skg5rldSek5OD+Ph49O7du9i0SqUSNjY2la7FHCq05+bEiRM4ffq0/v7WrVvRq1cvfPTRR6WOyiYiIqruHBwApVIcY1OS3FzxcQcH46+7Ro0aCAsLQ0JCAnJLKODevXvw9/fH1atXcfXqVX372bNnce/ePf2JPuXRsmVLnD9/Hs8991yxm4WFBZo1a4a///4bFy5cKHF+a2vrSv9UQlWoULgZOXKkfkMvX76M/v37w9bWFuvXr8ekSZOMWiAREZGp1K0LNG4sDhz+91EiQQD+/lscTFy3btWsPyEhAVqtFm3atMHGjRvx119/ITU1FYsWLUJwcDBCQ0MRGBiIQYMG4cSJEzh69CgiIiLQsWPHYoecyiI2NhZff/014uPj8X//939ITU3FDz/8gMmTJwMAOnbsiA4dOuCNN95AUlIS0tLS8PPPP2Pnzp0AAB8fH+Tk5CA5ORm3bt3Sn2xkbhUKNxcuXECLFi0AAOvXr0eHDh2wdu1aJCYmYuPGjcasj4iIyGQsLIDXXwdq1RIHD2dnAw8eiP+ePSu29+pl/MHERerXr48TJ06gc+fOGD9+PJo2bYouXbogOTkZS5cuhUwmw9atW+Hi4oIOHTogNDQU9evX15/YU15hYWHYtm0bdu3ahdatW+OFF17A559/jnr16umn2bhxI1q3bo0BAwYgICAAkyZN0u+tadeuHd555x2Eh4fD1dUVc+bMMUo/VFaFxtwIgqAfDb1792706NEDAODt7Y1bt24ZrzoiIiIT8/cXT/fevFkcXHztmngoqmVLMdhUxWngj/Lw8MDixYuxePHiEh+vW7cutm7dWur8U6dOLfF6M6UNWA4LC0NYWFipy6tRowZWrVpV6uNLly7F0qVL9ferw9lSFQo3QUFB+OSTTxAaGor9+/frNyotLQ1ubm5GLZCIiMjU/P3F070zMsTBww4O4qGoqtpjQ8ZVoXCzYMECDBo0CFu2bMHHH3+M5557DgCwYcMGtGvXzqgFEhERmYOFBeDjY+4qqCIqFG6aNWtmcLZUkblz5xpcCZGIiIjI1CoUbkpT9PsVREREROZSoXCj1Wrx+eef48cff0RGRkaxa9vcuXPHKMURERERlVeFhkbFx8dj/vz5CA8PR3Z2NqKjo9G7d29YWFhUm18EJSIiomdThcLNd999hxUrVmD8+PGwtLTEgAEDsHLlSsTGxup/kp2IiIjIHCoUbjIzMxEYGAhA/Hnz7OxsAECPHj2wfft241VHREREVE4VCjdeXl74559/AAANGjTArl27AADHjh2DQqEwXnVERERE5VShcPP6668jOTkZAPDee+9hypQp8PPzQ0REBIYOHWrUAomIiIjKo0JnS82ePVv/d3h4OOrWrYuUlBT4+fmhZ8+eRiuOiIjoWTJkyBDcu3cPW7ZsMXcpRtepUye0aNECCxYsqPJ1GeU6N8HBwQgODjbGooiIiMyroABQqwFHx+KPqVSAtbX4Y1MSpFarYW1tbdCm1Wohk8lg8RT99kSZK/3pp5/KfCMiInoqFRQA334LrF4t/hT4o7KzxfZvvxWnq2KdOnXCmDFjMGnSJNSoUQPu7u7FLrdy7949jBw5Em5ublAqlWjatCm2bdumf3zjxo1o0qQJFAoFfHx8MG/ePIP5fXx8MH36dERERMDR0REjRoxAYmIinJ2d8dNPPyEgIAAKhQIZGRkoLCzEhAkTUKdOHdjZ2aFt27bYt2+fwfIOHTqEl156CZ6enqhZsybCwsJw9+5dDBkyBPv378fChQshk8kgk8mQnp5eRT1Xjj03vXr1KtN0MplM/1PoRERETxW1GsjNBe7eBRITgSFDACcnMdgkJortRdOZYO/NmjVrEB0djSNHjiAlJQVDhgxBSEgIunTpAp1Oh27duuH+/fv49ttv0aBBA5w9e1b/M0jHjx9Hv379MHXqVISHh+Pw4cMYNWoUatasiSFDhujX8dlnnyE2NhZxcXEAgAMHDiAvLw+ffvopVq5ciZo1a6J27dqIiorC2bNn8cMPP8DT0xObN2/GK6+8gtOnT8PPzw8nT57Eyy+/jMjISEyfPh3Ozs7Yv38/tFotFi5ciAsXLqBp06aYNm0aAMDV1bXK+q3M4aY6/IQ5ERFRlXJ0FANNUZBJTAR69wY2bRLvu7iIj5d0yKoKNGvWTB86/Pz8sHjxYiQnJ6NLly7YvXs3jh49itTUVDRs2BAAUL9+ff288+fPx8svv4wpU6YAABo2bIizZ89i7ty5BuHmpZdewvjx4/X3Dxw4AI1GgyVLlqB58+YAgIyMDKxevRoZGRnw9PQEAEyYMAE7d+7E6tWrMXPmTMyZMwdBQUFISEiASqWCo6Oj/rIxAGBtbQ1bW1u4u7tXTWc9olwH0Pbs2YOAgACoVKpij2VnZ6NJkyY4cOCA0YojIiIyOScnMcC4uIiB5quvDIONk5PJSmnWrJnBfQ8PD9y4cQMAcPLkSXh5eemDzb+lpqYiJCTEoC0kJAR//fWXwRGWoKCgYvNaW1sbrPv06dPQarVo2LAh7O3t9bf9+/fj0qVL+npefvnlim2okZVrQPGCBQswfPhwOJaQWJ2cnDBy5EjMnz8f7du3N1qBREREJufkJO6x+eqrh229e5s02ACAlZWVwX2ZTKY/kmJjY2OUddjZ2RVrs7GxgUwm09/PycmBXC7H8ePH9Ye9itjb2xu1HmMo156bP//8E6+88kqpj3ft2hXHjx+vdFFERERmlZ0tHop61KZNxQcZm1GzZs3w999/48KFCyU+7u/vj0OHDhm0HTp0CA0bNiwWUJ7k+eefh1arxY0bN/Dcc88Z3IoOMzVr1kx/DbySWFtbm2xMbrnCTVZWVrEU+ShLS0vcvHmz0kURERGZzaODh11cgGHDHh6iSkysNgGnY8eO6NChA9544w0kJSUhLS0NP//8M3bu3AkAGD9+PJKTkzF9+nRcuHABa9asweLFizFhwoRyr6thw4YYNGgQIiIisGnTJqSlpeHo0aOYNWuW/meXYmJicOzYMYwePRpnzpzBuXPnsHTpUty6dQuAeGbWkSNHkJ6ejlu3blXpWN5yhZs6dergzJkzpT5+6tQpeHh4VLooIiIis1CpDIPNkCGAt7fhGJzERHG6amDjxo1o3bo1BgwYgICAAEyaNEm/d6Rly5b48ccf8cMPP6Bp06aIjY3FtGnTDAYTl8fq1asRERGB8ePHo1GjRujVqxeOHTuGunXrAhAD0K5du3Dq1CmEhoYiJCQEW7duhaWlOAJmwoQJkMvlCAgIgKurKzIyMozSByWRCYIglHXi9957D/v27cOxY8eg/NcpcPn5+WjTpg06d+6MRYsWGb1QY1GpVHByckJ2dnaJY4cqQ6PRYMeOHfjPf/7z2D1cVDnsZ9NgP5sG+9n4CgoKkJaWBl9fX4PvKp1Opz+Lp9QL0hVd5yY3t/jg4aI9OnZ2wJtvSvZCfpVVpn4uRWnPHVC+7+9yDSiePHkyNm3ahIYNGyIqKgqNGjUCAJw7dw4JCQnQarX4+OOPy7UhRERE1YZSKQaXkq5Q7OQEREZK+grFUlGucOPm5obDhw/j3XffRUxMDIp2+shkMoSFhSEhIQFubm5VUigREZFJKJWlhxcTXd+GKqfcvy1Vr1497NixA3fv3sXFixchCAL8/Pzg4uJSFfURERERlUuFfzjTxcUFrVu3NmYtRERERJX29PzEJxERUTmU43wZqiaM9Zwx3BARkaQUnXWWl5dn5kqovNRqNQCU+yKD/1bhw1JERETVkVwuh7Ozs/43mGxtbfU/W6BWq1FQUFDuU5Sp7CrazzqdDjdv3oStra3+2jgVxXBDRESSU/STAEUBBxAPeeTn5xf73SQyrsr0s4WFBerWrVvp54fhhoiIJEcmk8HDwwO1a9eGRqMBIF4w8ddff0WHDh14wcQqVJl+tra2NspeNYYbIiKSLLlcrh+/IZfL8eDBAyiVSoabKlQd+pkHHYmIiEhSGG6IiIhIUhhuiIiISFKqRbhJSEiAj48PlEol2rZti6NHj5Zpvh9++AEymQy9evWq2gKJiIjoqWH2cLNu3TpER0cjLi4OJ06cQPPmzREWFmZw+l5J0tPTMWHCBLRv395ElRIREdHTwOzhZv78+Rg+fDgiIyMREBCAZcuWwdbWFqtWrSp1Hq1Wi0GDBiE+Ph7169c3YbVERERU3Zk13KjVahw/fhyhoaH6NgsLC4SGhiIlJaXU+aZNm4batWtj2LBhpiiTiIiIniJmvc7NrVu3oNVq4ebmZtDu5uaGc+fOlTjPwYMH8dVXX+HkyZNlWkdhYSEKCwv191UqFQDxIkNFF3YylkcvFEVVh/1sGuxn02A/mw772jSqqp/Ls7yn6iJ+9+/fx1tvvYUVK1agVq1aZZpn1qxZiI+PL9a+a9cu2NraGrtEAEBSUlKVLJcMsZ9Ng/1sGuxn02Ffm4ax+7k8P4Rq1nBTq1YtyOVyZGVlGbRnZWXpfxfkUZcuXUJ6ejp69uypb9PpdAAAS0tLnD9/Hg0aNDCYJyYmBtHR0fr7KpUK3t7e6Nq1KxwdHY25OdBoNEhKSkKXLl149csqxH42DfazabCfTYd9bRpV1c9FR17KwqzhxtraGq1atUJycrL+dG6dTofk5GRERUUVm75x48Y4ffq0QdvkyZNx//59LFy4EN7e3sXmUSgUUCgUxdqtrKyq7MVdlcumh9jPpsF+Ng32s+mwr03D2P1cnmWZ/bBUdHQ0Bg8ejKCgILRp0wYLFixAbm4uIiMjAQARERGoU6cOZs2aBaVSiaZNmxrM7+zsDADF2omIiOjZZPZwEx4ejps3byI2NhaZmZlo0aIFdu7cqR9knJGRYZRfCCUiIqJng9nDDQBERUWVeBgKAPbt2/fYeRMTE41fEBERET21uEuEiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSlWoSbhIQE+Pj4QKlUom3btjh69Gip065YsQLt27eHi4sLXFxcEBoa+tjpiYiI6Nli9nCzbt06REdHIy4uDidOnEDz5s0RFhaGGzdulDj9vn37MGDAAOzduxcpKSnw9vZG165dce3aNRNXTkRERNWR2cPN/PnzMXz4cERGRiIgIADLli2Dra0tVq1aVeL03333HUaNGoUWLVqgcePGWLlyJXQ6HZKTk01cOREREVVHZg03arUax48fR2hoqL7NwsICoaGhSElJKdMy8vLyoNFoUKNGjaoqk4iIiJ4iluZc+a1bt6DVauHm5mbQ7ubmhnPnzpVpGR988AE8PT0NAtKjCgsLUVhYqL+vUqkAABqNBhqNpoKVl6xoecZeLhliP5sG+9k02M+mw742jarq5/Isz6zhprJmz56NH374Afv27YNSqSxxmlmzZiE+Pr5Y+65du2Bra1sldSUlJVXJcskQ+9k02M+mwX42Hfa1aRi7n/Py8so8rVnDTa1atSCXy5GVlWXQnpWVBXd398fO+9lnn2H27NnYvXs3mjVrVup0MTExiI6O1t9XqVT6QciOjo6V24B/0Wg0SEpKQpcuXWBlZWXUZdND7GfTYD+bBvvZdNjXplFV/Vx05KUszBpurK2t0apVKyQnJ6NXr14AoB8cHBUVVep8c+bMwYwZM/DLL78gKCjosetQKBRQKBTF2q2srKrsxV2Vy6aH2M+mwX42Dfaz6bCvTcPY/VyeZZn9sFR0dDQGDx6MoKAgtGnTBgsWLEBubi4iIyMBABEREahTpw5mzZoFAPj0008RGxuLtWvXwsfHB5mZmQAAe3t72Nvbm207iIiIqHowe7gJDw/HzZs3ERsbi8zMTLRo0QI7d+7UDzLOyMiAhcXDk7qWLl0KtVqNPn36GCwnLi4OU6dONWXpREREVA2ZPdwAQFRUVKmHofbt22dwPz09veoLIiIioqeW2S/iR0RERGRMDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKZbmLkAqCgqA5cuBevWAbt0ABwfAzg64cQP4+29AiQI0U+2DW+4lXJL5oUnNLPi8EYQBU/2hVKuAdeuAmjWBwkLAxgZ44QXA3d3cm0VloNMBly8D27cDCxcCmZliu5UVIAiATAYolWKbhYX4d04O8OCB+Lerq3j/7t2Hy5TJALUa0GoBuVycVq0W2x0cAC8vsU0mA7KzxeUWza9QAE2bAj4+gKWl+Bq8fBm4f1+8r9OJr1dLS8DaGsjPF9vs7MR/tVpxec7OwPTp4nJUKnFbbGzE7dJqAY0GsLUFPDzE25Ur4mtdoxFrdnUF3NwM11FQANy6Bdy+LS5PLhe3R6MRH5fLgVq1AG/vh+v4+29xPhsbsb/y88V6bGzEdXh5idPcuCHOI5OJ/VnUV87OYt9ZW4vrvnNH7Dtra6B+ffFfmQy4eVOcrmhemUzsS0tLsS6ZTKy5sFC8r1SK6yvqfw8Psd6cnId9mJ398HlSKsV1CYLYh/b2Yn1OTsDEiUBUlLht166J26JUivUplcDx48DVqw/71sVFfK7z8sQ2CwuxVqXyYT8BYj9ptWINeXnia0AuBxo3BlatAlq0EOelp8D588CFC0CdOuKTfvMmcP060KYNcPq0+CawtRW/R5KSHn4Q5OaKT7yzszhPerr4QvP3F1+YPj7isuVyICNDfEG0bg34+QH37gEHDwINGz6cv1YtoHZt4MAB8YXVrp34pkhPBwICxBff7dvii/TXX8UXs6cn0Ly5uAxTEaqBxYsXC/Xq1RMUCoXQpk0b4ciRI4+d/scffxQaNWokKBQKoWnTpsL27dvLvK7s7GwBgJCdnV3ZsvWmTBEEW1tBsLFRC1u2bBFsbNSC+BEm3hTIF6ZjoqCBTNACggYQHgDCZdQV2skPC3/49xUMZpDJBCEwUBD++cdoNUqJWi32s1qtNncpwtmzgvDWW4JgYWH4FErhVtrrmTfp9LOrq/gaflZUp8+Ocjl3ThBq1hQEuVwQ7OzEL5yHLyDzv4j/dVPb2Ij9bGcnfp9ZWgrCa68Jwt27leqG8nx/mz2zr1u3DtHR0YiLi8OJEyfQvHlzhIWF4caNGyVOf/jwYQwYMADDhg3DH3/8gV69eqFXr144c+aMiSsXxcYCM2eK/ysqjTXU8MbfsIAAGQA5xOOB9ZCB7dqX0Sx1PYRHZxAE8b9cj/5Xnqqd1FRg/Hjg22/FvQ5ET5ubN4GQEPG1TNXY9evi3hGtVtwT8+gXTn6++ep6Ep1O/D7TasXdpjk5Jlu12cPN/PnzMXz4cERGRiIgIADLli2Dra0tVq1aVeL0CxcuxCuvvIKJEyfC398f06dPR8uWLbF48WITVy7uKl+8WHzeHuc+HDEGS7EWfQ1CjAyAE/Ih+999/WN16wLbtom7Dala0umADRuAI0fE9y7R0+ruXeC77xjQq7XOnYFNm8RjmU8bmQwIDgbWrhWPIZuIWcfcqNVqHD9+HDExMfo2CwsLhIaGIiUlpcR5UlJSEB0dbdAWFhaGLVu2lDh9YWEhCgsL9fdVKhUAQKPRQKPRVKr+VavEY/Q2NuJ9GxuNwb+P0sAW47EUgCX6Yos+0DxKAGDh7Q2sXw80aiQeV6Viip63yj5/lZGRIQaboiFSUvS41zMZT3Xo52++ASIjxf9XSVl1+OyosE6dgI0bgTffNOkekIrQ/O9DUWNrC7RtK35ZFg1Kq8xyyzG/TBDM9//O69evo06dOjh8+DCCg4P17ZMmTcL+/ftx5MiRYvNYW1tjzZo1GDBggL5tyZIliI+PR1ZWVrHpp06divj4+GLta9euha2trZG2hIiIiKpSXl4eBg4ciOzsbDg6Oj52WsmfLRUTE2Owp0elUsHb2xtdu3Z9Yuc8yYoVwKRJD3fn2thosGpVEoYO7YL8fKti0ztAhXl4r2x7bnhIqlQajQZJSUno0qULrKyK97MpZGQAY8cC+/ZJd3f+k17PZBzVoZ9dXYE9e56NPTfm/uyolL17n5o9N0mrVqHLsGGwCgwU99wY4ZBU0ZGXsjBruKlVqxbkcnmxPS5ZWVlwL+U0aHd393JNr1AooFAoirVbWVlV+sU9dCgweXLxcb/5+VbFPqQckY15eBcDsb7EYFNEduEC0LOneF5x06aVqk/qjPEcVpSvr7i3NSVFPLVYykp6PZPxmbOf33pLfE0/K6eFm/Ozo8KSkoA33qj2weZRVnl5sNq/Xwxk339f6fRcnufMrC9la2trtGrVCsnJyfo2nU6H5ORkg8NUjwoODjaYHgCSkpJKnb4qKZXitSnk8sdP5wAVFpUQbAQA2bDRDyTWP5aRAfTowVMYqjELC6BPHzHgyB6XVomqORcXYNCgZyfYPJX27gV6936qgo2eIIj/Cxw4ULwWj4mY/eUcHR2NFStWYM2aNUhNTcW7776L3NxcREZGAgAiIiIMBhyPHTsWO3fuxLx583Du3DlMnToVv//+O6KiosxS/7RpwEcfiddOKo0a1rgKL+gggwBAC0AH4Arqors8Gaf8+xruzZHJAEfHh1fqomrJ3x+YN0/8Twm/GOhpVLs2cOgQj4JXe56e4kX55HLxapuPfuFU5zMaLCzE77Oiq3Oa8Gwvs4+5CQ8Px82bNxEbG4vMzEy0aNECO3fuhJubGwAgIyMDFo98c7Rr1w5r167F5MmT8dFHH8HPzw9btmxBUzMewikKOMuXF9X47ysUK7EJ03BR9VKxKxQnT/WHUt0EWNeFVyh+Cvn7A4mJ4vWOpHiFYkDM2HK5uC28QnHVXKEYEA8N8QrFVKJGjcQU+jRdoRgAtm412xWKzXq2lDmoVCo4OTmVabR1eWk0GuzYsQP/+c9/nr7juU8R9rNpsJ9Ng/1sOuxr06iqfi7P9zczOxEREUkKww0RERFJCsMNERERSQrDDREREUkKww0RERFJCsMNERERSQrDDREREUkKww0RERFJCsMNERERSYrZf37B1IouyFyen04vK41Gg7y8PKhUKl79sgqxn02D/Wwa7GfTYV+bRlX1c9H3dll+WOGZCzf3798HAHh7e5u5EiIiIiqv+/fvw6noR9lK8cz9tpROp8P169fh4OAAmUz25BnKQaVSwdvbG1evXjX671bRQ+xn02A/mwb72XTY16ZRVf0sCALu378PT09Pgx/ULskzt+fGwsICXl5eVboOR0dHvnFMgP1sGuxn02A/mw772jSqop+ftMemCAcUExERkaQw3BAREZGkMNwYkUKhQFxcHBQKhblLkTT2s2mwn02D/Ww67GvTqA79/MwNKCYiIiJp454bIiIikhSGGyIiIpIUhhsiIiKSFIYbIiIikhSGGyNJSEiAj48PlEol2rZti6NHj5q7JMmZNWsWWrduDQcHB9SuXRu9evXC+fPnzV2W5M2ePRsymQzjxo0zdymSc+3aNbz55puoWbMmbGxsEBgYiN9//93cZUmKVqvFlClT4OvrCxsbGzRo0ADTp08v0+8T0eP9+uuv6NmzJzw9PSGTybBlyxaDxwVBQGxsLDw8PGBjY4PQ0FD89ddfJqmN4cYI1q1bh+joaMTFxeHEiRNo3rw5wsLCcOPGDXOXJin79+/H6NGj8dtvvyEpKQkajQZdu3ZFbm6uuUuTrGPHjuHLL79Es2bNzF2K5Ny9exchISGwsrLCzz//jLNnz2LevHlwcXExd2mS8umnn2Lp0qVYvHgxUlNT8emnn2LOnDn44osvzF3aUy83NxfNmzdHQkJCiY/PmTMHixYtwrJly3DkyBHY2dkhLCwMBQUFVV+cQJXWpk0bYfTo0fr7Wq1W8PT0FGbNmmXGqqTvxo0bAgBh//795i5Fku7fvy/4+fkJSUlJQseOHYWxY8eauyRJ+eCDD4QXX3zR3GVIXvfu3YWhQ4catPXu3VsYNGiQmSqSJgDC5s2b9fd1Op3g7u4uzJ07V9927949QaFQCN9//32V18M9N5WkVqtx/PhxhIaG6tssLCwQGhqKlJQUM1YmfdnZ2QCAGjVqmLkSaRo9ejS6d+9u8Nom4/npp58QFBSEvn37onbt2nj++eexYsUKc5clOe3atUNycjIuXLgAAPjzzz9x8OBBdOvWzcyVSVtaWhoyMzMNPj+cnJzQtm1bk3w3PnM/nGlst27dglarhZubm0G7m5sbzp07Z6aqpE+n02HcuHEICQlB06ZNzV2O5Pzwww84ceIEjh07Zu5SJOvy5ctYunQpoqOj8dFHH+HYsWMYM2YMrK2tMXjwYHOXJxkffvghVCoVGjduDLlcDq1WixkzZmDQoEHmLk3SMjMzAaDE78aix6oSww09lUaPHo0zZ87g4MGD5i5Fcq5evYqxY8ciKSkJSqXS3OVIlk6nQ1BQEGbOnAkAeP7553HmzBksW7aM4caIfvzxR3z33XdYu3YtmjRpgpMnT2LcuHHw9PRkP0sYD0tVUq1atSCXy5GVlWXQnpWVBXd3dzNVJW1RUVHYtm0b9u7dCy8vL3OXIznHjx/HjRs30LJlS1haWsLS0hL79+/HokWLYGlpCa1Wa+4SJcHDwwMBAQEGbf7+/sjIyDBTRdI0ceJEfPjhh+jfvz8CAwPx1ltv4f3338esWbPMXZqkFX3/meu7keGmkqytrdGqVSskJyfr23Q6HZKTkxEcHGzGyqRHEARERUVh8+bN2LNnD3x9fc1dkiS9/PLLOH36NE6ePKm/BQUFYdCgQTh58iTkcrm5S5SEkJCQYpcyuHDhAurVq2emiqQpLy8PFhaGX3VyuRw6nc5MFT0bfH194e7ubvDdqFKpcOTIEZN8N/KwlBFER0dj8ODBCAoKQps2bbBgwQLk5uYiMjLS3KVJyujRo7F27Vps3boVDg4O+uO2Tk5OsLGxMXN10uHg4FBsHJOdnR1q1qzJ8U1G9P7776Ndu3aYOXMm+vXrh6NHj2L58uVYvny5uUuTlJ49e2LGjBmoW7cumjRpgj/++APz58/H0KFDzV3aUy8nJwcXL17U309LS8PJkydRo0YN1K1bF+PGjcMnn3wCPz8/+Pr6YsqUKfD09ESvXr2qvrgqPx/rGfHFF18IdevWFaytrYU2bdoIv/32m7lLkhwAJd5Wr15t7tIkj6eCV43//ve/QtOmTQWFQiE0btxYWL58ublLkhyVSiWMHTtWqFu3rqBUKoX69esLH3/8sVBYWGju0p56e/fuLfEzefDgwYIgiKeDT5kyRXBzcxMUCoXw8ssvC+fPnzdJbTJB4GUaiYiISDo45oaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiIiIJIXhhoiIiCSF4YaIiIgkheGGiKgSOnXqhHHjxpm7DCJ6BMMNEZVoyJAhkMlkxW6PXm69MhITE+Hs7GyUZVVEz5498corr5T42IEDByCTyXDq1CkTV0VExsBwQ0SleuWVV/DPP/8Y3KrjD5ZqNJpyzzNs2DAkJSXh77//LvbY6tWrERQUhGbNmhmjPCIyMYYbIiqVQqGAu7u7wa3oV8G3bt2Kli1bQqlUon79+oiPj8eDBw/0886fPx+BgYGws7ODt7c3Ro0ahZycHADAvn37EBkZiezsbP0eoalTpwIAZDIZtmzZYlCHs7MzEhMTAQDp6emQyWRYt24dOnbsCKVSie+++w4AsHLlSvj7+0OpVKJx48ZYsmRJqdvWo0cPuLq66pdbJCcnB+vXr8ewYcNw+/ZtDBgwAHXq1IGtrS0CAwPx/fffP7bPnlQ/AFy9ehX9+vWDs7MzatSogddeew3p6emPXS4RlR3DDRGV24EDBxAREYGxY8fi7Nmz+PLLL5GYmIgZM2bop7GwsMCiRYvwf//3f1izZg327NmDSZMmAQDatWuHBQsWwNHRUb9HaMKECeWq4cMPP8TYsWORmpqKsLAwfPfdd4iNjcWMGTOQmpqKmTNnYsqUKVizZk2J81taWiIiIgKJiYl49Cf21q9fD61WiwEDBqCgoACtWrXC9u3bcebMGYwYMQJvvfUWjh49WoFeE2k0GoSFhcHBwQEHDhzAoUOHYG9vj1deeQVqtbrCyyWiR5jk5zmJ6KkzePBgQS6XC3Z2dvpbnz59BEEQhJdfflmYOXOmwfTffPON4OHhUery1q9fL9SsWVN/f/Xq1YKTk1Ox6QAImzdvNmhzcnLS//p7WlqaAEBYsGCBwTQNGjQQ1q5da9A2ffp0ITg4uNSaUlNTBQDC3r179W3t27cX3nzzzVLn6d69uzB+/Hj9/X//YvqT6v/mm2+ERo0aCTqdTv94YWGhYGNjI/zyyy+lrpeIys7SvNGKiKqzzp07Y+nSpfr7dnZ2AIA///wThw4dMthTo9VqUVBQgLy8PNja2mL37t2YNWsWzp07B5VKhQcPHhg8XllBQUH6v3Nzc3Hp0iUMGzYMw4cP17c/ePAATk5OpS6jcePGaNeuHVatWoVOnTrh4sWLOHDgAKZNm6bfppkzZ+LHH3/EtWvXoFarUVhYWKn6//zzT1y8eBEODg4G7QUFBbh06VKFl0tEDzHcEFGp7Ozs8NxzzxVrz8nJQXx8PHr37l3sMaVSifT0dPTo0QPvvvsuZsyYgRo1auDgwYMYNmwY1Gr1Y8OBTCYzOEwElDxguChoFdUDACtWrEDbtm0NpisaI1SaYcOG4b333kNCQgJWr16NBg0aoGPHjgCAuXPnYuHChViwYIF+/NC4ceMee/joSfXn5OSgVatW+nFCj3J1dX1srURUNgw3RFRuLVu2xPnz50sMPgBw/Phx6HQ6zJs3DxYW4tC+H3/80WAaa2traLXaYvO6urrin3/+0d//66+/kJeX99h63Nzc4OnpicuXL2PQoEHl2pZ+/fph7NixWLt2Lb7++mu8++67kMlkAIBDhw7htddew5tvvgkA0Ol0uHDhAgICAkpd3pPqb9myJdatW4fatWvD0dGxXLUSUdkw3BBRucXGxqJHjx6oW7cu+vTpAwsLC/z55584c+YMPvnkEzz33HPQaDT44osv0LNnTxw6dAjLli0zWIaPjw9ycnKQnJyM5s2bw9bWFra2tnjppZewePFiBAcHQ6vV4oMPPoCVldUTa4qPj8eYMWPg5OSEV155BYWFhfj9999x9+5dREdHlzqfvb09wsPDERMTA5VKhSFDhugf8/Pzw4YNG3D48GG4uLhg/vz5yMrKemy4eVL9gwYNwty5c/Haa69h2rRp8PLywpUrV7Bp0yZMmjQJXl5eT9xWIno8ni1FROUWFhaGbdu2YdeuXWjdujVeeOEFfP7556hXrx4AoHnz5pg/fz4+/fRTNG3aFN999x1mzZplsIx27drhnXfeQXh4OFxdXTFnzhwAwLx58+Dt7Y327dtj4MCBmDBhQpnGuLz99ttYuXIlVq9ejcDAQHTs2BGJiYllui7PsGHDcPfuXYSFhcHT01PfPnnyZLRs2RJhYWHo1KkT3N3d0atXr8cu60n129ra4tdff0XdunXRu3dv+Pv7Y9iwYSgoKOCeHCIjkQn/PjhMRERE9BTjnhsiIiKSFIYbIiIikhSGGyIiIpIUhhsiIiKSFIYbIiIikhSGGyIiIpIUhhsiIiKSFIYbIiIikhSGGyIiIpIUhhsiIiKSFIYbIiIikhSGGyIiIpKU/wdgOENVTxe+WgAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3_A - (Non-Linear Data) Extra Credit - General Deep Learning\n",
    "\n",
    "The goal of this assignment is to modify the General Deep Learning network to implement classification tasks using a cross-entropy loss and a logistic activation function in the output layer. You are required to train with a 1-dimesional Non-Linear Data.\n",
    "\n",
    "## 3.1 Network Architecture\n",
    "\n",
    "- The simple linear regression can be visualized as a neural network:\n",
    "  - **Input Layer**: It accepts the input feature.\n",
    "  - **Hidden Layer**: Contains one unit which simply passes the value.\n",
    "  - **Bias Layer**: An added constant to introduce flexibility to the model.\n",
    "  - **ReLU Activation**: Introduces non-linearity into the model. This means that the model using ReLU can be trained to approximate complex, non-linear functions, which is essential for learning from the data.\n",
    "  - **Output Layer**: Produces the prediction for the given input.\n",
    "\n",
    "Mathematically, the output of the network is represented as:\n",
    "\n",
    "$$ \\ z^{(L)} = W^{(L)} a^{(L-1)} + b^{(L)} \\ $$\n",
    "$$ \\ a^{(L)} = \\sigma(z^{(L)}) \\ $$\n",
    "$$ \\ a^{(L)} = \\frac{1}{1 + e^{-z^{(L)}}} \\ $$\n",
    "<!-- $$ \\ a_2 = \\sum_{k=1}^{d} Xw_{2k} a_{1k} + b_2 \\ $$\n",
    "$$ OR $$\n",
    "$$ \\hat{y} = g(\\mathcal{H^{l}} \\mathcal{W^{(l+1)}} + \\mathcal{b^{(l+1)}}) $$ -->\n",
    "\n",
    "Expected output for your predictions should look like the plot below:\n",
    "\n",
    "![3ecnl.png](attachment:3ecnl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is for importing libraries needed for this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ###### \n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "# This Cell Imports all the required Libraries.\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import collections.abc\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Data Utility Functions\n",
    "\n",
    "This section provides utility functions to generate and manipulate data, mainly focusing on data splitting and data generation.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. `train_test_split` Function:\n",
    "\n",
    "This function is responsible for splitting the given data into training and testing sets based on the specified `test_size` fraction.\n",
    "\n",
    "- **Input**:\n",
    "  - `x`: Feature data.\n",
    "  - `y`: Target labels.\n",
    "  - `test_size`: Fraction of the dataset to be used as testing data (default is 0.2, i.e., 20%).\n",
    "\n",
    "- **Output**:\n",
    "  - Training and testing data for both features and labels.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. `generate_sine_data` Function:\n",
    "\n",
    "Generates data based on the sine function. This function simulates a non-linear relationship.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `num_samples`: Number of data points to generate.\n",
    "  - `dimension`: Dimension of the input data.\n",
    "  - `amplitude`, `frequency`, `phase`: Parameters of the sine function.\n",
    "  - `categorical`: If set to `True`, the output is binarized based on the median value.\n",
    "  \n",
    "- **Output**:\n",
    "  - Training and testing data for both features and labels.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. `generate_data` Function:\n",
    "\n",
    "Generates linear data with optional noise. This function simulates a linear relationship.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `num_samples`: Number of data points to generate.\n",
    "  - `dimension`: Dimension of the input data.\n",
    "  - `m` and `b`: Slope and intercept of the linear relationship.\n",
    "  - `categorical`: If set to `True`, the output is binarized based on the median value.\n",
    "  \n",
    "- **Output**:\n",
    "  - Training and testing data for both features and labels.\n",
    "\n",
    "---\n",
    "\n",
    "#### Utility Functions:\n",
    "\n",
    "- `q3_b_l` and `q3_b_nl`: \n",
    "  - These functions are wrappers around the data generating functions. They ensure that data is correctly formatted and split before being used in the subsequent processing.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: The main intent behind these utility functions is to ease the data generation and preparation steps. You do not need to edit these functions for most tasks. They're here to ensure you have the necessary data structure for your experiments.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ###### \n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "\n",
    "def train_test_split(x, y, test_size=0.2):\n",
    "\n",
    "    # Split data\n",
    "    num_samples = len(x)\n",
    "    num_test = int(test_size * num_samples)\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    test_indices = indices[:num_test]\n",
    "    train_indices = indices[num_test:]\n",
    "\n",
    "    x_train = [x[i] for i in train_indices]\n",
    "    x_test = [x[i] for i in test_indices]\n",
    "\n",
    "    y_train = [y[i] for i in train_indices] \n",
    "    y_test = [y[i] for i in test_indices]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "  \n",
    "\n",
    "def generate_sine_data(num_samples=100, dimension=1, test_size=0.2, amplitude=1, frequency=1, phase=0, low=0, high=10, categorical=False):\n",
    "    x = np.random.uniform(low=low, high=high, size=(num_samples, dimension))\n",
    "\n",
    "    if dimension == 1:\n",
    "        y = amplitude * np.sin(frequency * x + phase)\n",
    "    else:\n",
    "        y = np.zeros((num_samples, 1))\n",
    "        for d in range(dimension):\n",
    "            y += amplitude * np.sin(frequency * x[:, d].reshape(-1, 1) + phase)\n",
    "\n",
    "    if categorical:\n",
    "        median = np.median(y)\n",
    "        labels = np.zeros(num_samples, dtype=int)\n",
    "        labels[y[:, 0] <= median] = 0\n",
    "        labels[y[:, 0] > median] = 1\n",
    "        y = labels\n",
    "        y = y.astype(int)\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size)\n",
    "\n",
    "    return np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)\n",
    "\n",
    "def generate_data(num_samples=100, dimension=1, test_size=0.2, m=7, b=3, low=0, high=10, categorical=False):\n",
    "\n",
    "    # Generate x values\n",
    "    x = np.random.uniform(low=low, high=high, size=(num_samples, dimension))\n",
    "\n",
    "    # Compute y values with noise\n",
    "    if dimension == 1:\n",
    "        # noise = np.random.normal(loc=0, scale=1, size=num_samples)\n",
    "        noise = np.random.normal(loc=0, scale=1, size=(num_samples, 1))\n",
    "        y = (m * x) + b + noise\n",
    "    \n",
    "    else:\n",
    "        noise = np.random.normal(loc=0, scale=1, size=num_samples)\n",
    "        y = np.dot(x, np.array([m] * dimension)) + b + noise\n",
    "\n",
    "    y = y.reshape(-1, 1)  # Make y a column vector\n",
    "\n",
    "    if categorical:\n",
    "        median = np.median(y)\n",
    "        labels = np.zeros(num_samples, dtype=int)\n",
    "        labels[y[:, 0] <= median] = 0\n",
    "        labels[y[:, 0] > median] = 1\n",
    "        print(median)\n",
    "        y = labels\n",
    "        y = y.astype(int)\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "    # Split data into train/test\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size)\n",
    "\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "def q3_a_l():\n",
    "    xtrain, xtest, ytrain, ytest = generate_data(num_samples=10000, test_size=0.2, categorical=True)\n",
    "    xtrain, xtest, ytrain, ytest = np.array(xtrain, dtype=np.float32), np.array(xtest, dtype=np.float32), np.array(ytrain, dtype=np.float32), np.array(ytest, dtype=np.float32)\n",
    "    print(xtrain.shape, xtest.shape, ytrain.shape, ytest.shape)\n",
    "    # print(xtrain)\n",
    "    # print(ytrain)\n",
    "    return {\n",
    "        \"train\": (xtrain, ytrain),\n",
    "        \"test\": (xtest, ytest)\n",
    "    }\n",
    "\n",
    "def q3_a_nl():\n",
    "    xtrain, xtest, ytrain, ytest = generate_sine_data(num_samples=10000, dimension=1, test_size=0.2,categorical=True)\n",
    "    xtrain, xtest, ytrain, ytest = np.array(xtrain, dtype=np.float32), np.array(xtest, dtype=np.float32), np.array(ytrain, dtype=np.float32), np.array(ytest, dtype=np.float32)\n",
    "    print(xtrain.shape, xtest.shape, ytrain.shape, ytest.shape)\n",
    "    # print(xtrain)\n",
    "    # print(ytrain)\n",
    "    return {\n",
    "        \"train\": (xtrain, ytrain),\n",
    "        \"test\": (xtest, ytest)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `LinearLayer` Class: Fully Connected Layer\n",
    "\n",
    "This class represents a fully connected (or linear) layer within a neural network. This is the foundation of dense layers in many deep learning models, where every neuron in the current layer is connected to every neuron in the next layer.\n",
    "\n",
    "---\n",
    "\n",
    "#### Attributes:\n",
    "\n",
    "- **`input_layer`**: The preceding layer in the neural network.\n",
    "- **`output_dimension`**: Specifies the number of neurons in the current layer.\n",
    "- **`W`**: Weights matrix that will be learned during training.\n",
    "\n",
    "---\n",
    "\n",
    "#### Methods:\n",
    "\n",
    "1. **`__init__(self, input_layer, number_out_features)`**: \n",
    "    - Initializes the layer based on the dimensions of the input layer and the desired number of output features.\n",
    "    - Ensures that the input layer contains a list of 1D linear feature data.\n",
    "    - Initializes weights randomly.\n",
    "\n",
    "2. **`forward(self)`**:\n",
    "    - Computes the forward pass for the layer, essentially multiplying the input data by the weight matrix (`XW`).\n",
    "\n",
    "3. **`backward(self, downstream)`**:\n",
    "    - Computes the backward pass, propagating the gradient backward to the input layer and adjusting weights based on the propagated gradient.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: Care has been taken to ensure that the weights are not initialized with NaN or Infinity values, and this is checked again during the forward pass. It's essential to ensure that these do not propagate through the neural network, as they would lead to unstable training dynamics.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ###### \n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "\n",
    "class LinearLayer:\n",
    "    \"\"\"\n",
    "    A class representing a fully connected (linear) layer in a neural network.\n",
    "    \n",
    "    Methods:\n",
    "        forward(): Computes the forward pass of the layer.\n",
    "        backward(dwnstrm): Computes the backward pass, propagating the gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_layer, number_out_features) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the layer.\n",
    "\n",
    "        Parameters:\n",
    "            input_layer: The preceding layer in the neural network.\n",
    "            output_dimension: Number of neurons in the current layer.\n",
    "        \n",
    "        Raises:\n",
    "            AssertionError: If input layer dimensions are not a list of 1D linear feature data.\n",
    "        \"\"\"\n",
    "        assert len(input_layer.output_dimension) == 2, \"Input layer must contain a list of 1D linear feature data.\"\n",
    "        self.input_layer = input_layer\n",
    "        num_data, num_in_features = input_layer.output_dimension\n",
    "        self.output_dimension = np.array([num_data, number_out_features])\n",
    "        assert num_in_features > 0, \"num_in_features should be greater than 0\"\n",
    "        np.random.seed(42)\n",
    "\n",
    "        self.W = np.random.randn(num_in_features, number_out_features) / np.sqrt(num_in_features)\n",
    "        assert not np.isnan(self.W).any(), \"Initial weights contain NaN\"\n",
    "        assert not np.isinf(self.W).any(), \"Initial weights contain Inf\"\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Compute the forward pass for the layer, i.e., compute XW.\n",
    "        \"\"\"\n",
    "        self.input_array = self.input_layer.forward()\n",
    "        assert not np.isnan(self.input_array).any(), \"Input array contains NaN\"\n",
    "        assert not np.isnan(self.W).any(), \"Weights contain NaN before update\"\n",
    "        assert not np.isinf(self.W).any(), \"Weights contain Inf before update\"\n",
    "\n",
    "        self.output_array = self.input_array @ self.W\n",
    "        return self.output_array\n",
    "\n",
    "    def backward(self, downstream):\n",
    "        \"\"\"\n",
    "        Compute the backward pass for the layer, propagating the gradient backward.\n",
    "        \"\"\"\n",
    "        self.G = self.input_array[:, :, np.newaxis] * downstream[:, np.newaxis]\n",
    "\n",
    "        input_grad = (self.W @ downstream[:, :, np.newaxis]).squeeze(axis=-1)\n",
    "        self.input_layer.backward(input_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `HiddenLayer` Class\n",
    "\n",
    "`HiddenLayer` is a class that models a hidden layer in a neural network. Building on the foundation of the `LinearLayer` class, it introduces activation functionality to it. As the name suggests, this layer is generally used in the middle layers of deep neural networks to enable non-linear transformations of the input data.\n",
    "\n",
    "---\n",
    "\n",
    "#### Methods:\n",
    "\n",
    "1. **Constructor: `__init__(self, input_dimension, output_dimension)`**\n",
    "    - **Purpose**: Initializes the HiddenLayer.\n",
    "    - **Parameters**:\n",
    "        - `input_dimension`: Specifies the number of input features.\n",
    "        - `output_dimension`: Dictates the number of output features or neurons in the hidden layer.\n",
    "    - **Behavior**: Calls the constructor of the parent `LinearLayer` class to handle weight initialization and set up dimensions.\n",
    "\n",
    "2. **`forward(self)`**:\n",
    "    - **Purpose**: Handles the forward propagation of data in the neural network.\n",
    "    - **Behavior**: Takes input data and conducts a linear transformation. In this implementation, an activation function's operation is implied but not explicitly included. For real-world cases, an activation like ReLU, Sigmoid, etc., would be applied to the output of this function.\n",
    "    - **Returns**: \n",
    "        - `_out`: The linearly transformed data.\n",
    "\n",
    "3. **`backward(self, downstream)`**:\n",
    "    - **Purpose**: Handles the backward propagation, which is essential for training neural networks using gradient-based optimization algorithms.\n",
    "    - **Parameters**:\n",
    "        - `downstream`: Represents the gradient of the loss concerning the output of this layer.\n",
    "    - **Behavior**: Computes the gradient concerning the inputs and weights of this layer by considering both the gradient of the activation function and the linear transformation.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: The utility of this layer comes into full effect when combined with non-linear activation functions. This combination allows neural networks to capture and model more complex and nuanced relationships in the data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ###### \n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "\n",
    "class HiddenLayer(LinearLayer):\n",
    "    \"\"\"\n",
    "    Represents a hidden layer in a neural network. Inherits from the LinearLayer class\n",
    "    and adds activation functionality.\n",
    "    \n",
    "    Methods:\n",
    "    --------\n",
    "    forward(): Performs the forward pass, including linear transformation and activation.\n",
    "    backward(downstream): Performs the backward pass, including both activation and linear gradients.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dimension, output_dimension) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the HiddenLayer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dimension: The number of input features.\n",
    "        output_dimension: The number of output features.\n",
    "        \"\"\"\n",
    "        super().__init__(input_dimension, output_dimension)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the forward pass by first conducting the linear transformation and then the activation.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        _out: The linearly transformed data.\n",
    "        \"\"\"\n",
    "        _out = super().forward()\n",
    "        return _out\n",
    "    \n",
    "    def backward(self, downstream):\n",
    "        \"\"\"\n",
    "        Performs the backward pass by propagating the gradient through the activation function\n",
    "        and then through the linear transformation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        downstream: The gradient of the loss with respect to the output of this layer.\n",
    "        \"\"\"\n",
    "        super().backward(downstream=downstream)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `LinearActivation` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "The `LinearActivation` class implements the linear activation function. This is essentially the identity function: for any input `x`, the output will be `x`. This activation is typically used in the output layer for regression tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods\n",
    "\n",
    "#### `forward(input_array: np.ndarray) -> np.ndarray`\n",
    "- **Purpose**: Computes the forward pass for linear activation.\n",
    "- **Parameters**: `input_array`: Input data or activations from the previous layer.\n",
    "- **Returns**: The same as `input_array`, because the linear activation doesn't change its input.\n",
    "- **Description**: For the linear activation, the output is the same as the input.\n",
    "\n",
    "---\n",
    "\n",
    "#### `backward(downstream: np.ndarray, input_array=None) -> np.ndarray`\n",
    "- **Purpose**: Computes the backward pass (gradient) for linear activation.\n",
    "- **Parameters**: \n",
    "  - `downstream`: The gradient of the loss function with respect to the output of the linear activation.\n",
    "- **Returns**: Gradient of the loss with respect to the input of the linear activation.\n",
    "- **Description**: The derivative of the linear function is 1, so this operation multiplies it by the downstream gradient. However, since the derivative is 1, this operation doesn't change the downstream gradient.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ###### \n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "\n",
    "class LinearActivation:\n",
    "    \"\"\"\n",
    "    Implements the linear activation function. \n",
    "\n",
    "    This activation is essentially the identity function. For any input 'x', the output will be 'x'.\n",
    "    It's commonly used in the output layer for regression tasks.\n",
    "\n",
    "    Methods:\n",
    "        forward(): Computes the forward pass by simply returning the input.\n",
    "        backward(): Computes the backward pass (derivative) which is 1 for the linear activation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(input_array):\n",
    "        \"\"\"\n",
    "        Computes the forward pass for linear activation.\n",
    "\n",
    "        Parameters:\n",
    "            input_array (np.ndarray): Input data or activations from the previous layer.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Same as input_array, as the linear activation doesn't change its input.\n",
    "        \"\"\"\n",
    "        # For linear activation, the output is same as input\n",
    "        return input_array\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(downstream, input_array=None):\n",
    "        \"\"\"\n",
    "        Computes the backward pass (gradient) for linear activation.\n",
    "\n",
    "        Parameters:\n",
    "            downstream_gradient (np.ndarray): The gradient of the loss function with respect to the output of the linear activation.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the input of the linear activation.\n",
    "        \"\"\"\n",
    "        # The derivative of the linear function is 1, so we just multiply it by the downstream gradient\n",
    "        # However, since the derivative is 1, this operation doesn't change the downstream gradient.\n",
    "        return 1 * downstream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `SigmoidActivation` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "The `SigmoidActivation` class implements the Sigmoid activation function. The sigmoid function is defined as: \n",
    "$$\\ f(z) = \\frac{1}{1 + e^{-z}} \\ $$\n",
    "where:\n",
    "- \\( z \\) is the input\n",
    "- \\( e \\) is the base of natural logarithms (approximately equal to 2.71828)\n",
    "\n",
    "### Attributes:\n",
    "- `input_layer`: The layer that provides input to this activation function.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `forward(input_array: np.ndarray) -> np.ndarray`\n",
    "- **Purpose**: Applies the Sigmoid activation function to the output of the input layer.\n",
    "- **Parameters**: `input_array`: Array of inputs to be passed through the sigmoid activation function.\n",
    "- **Returns**: The output after applying the sigmoid activation.\n",
    "- **Description**: \n",
    "  - Computes the negative exponential for every element in `input_array` using `np.exp()`.\n",
    "  - Calculates the denominator by adding 1 to every element of the previously computed negative exponential.\n",
    "  - Divides 1 by the computed denominator to get the sigmoid value.\n",
    "\n",
    "---\n",
    "\n",
    "#### `backward(downstream: np.ndarray, input_array: np.ndarray) -> np.ndarray`\n",
    "- **Purpose**: Computes the gradient of the loss with respect to the input, and passes this back to the previous layers.\n",
    "- **Parameters**: \n",
    "  - `downstream`: The gradient of the loss with respect to the output of the sigmoid function.\n",
    "  - `input_array`: The original input to the sigmoid function.\n",
    "- **Returns**: Gradient of the loss with respect to the input of the sigmoid function.\n",
    "- **Description**: \n",
    "  - Uses the sigmoid formula from the forward function to get the sigmoid value.\n",
    "  - Computes the derivative of the sigmoid function using the formula: \n",
    "  $$ \\ ( f'(z) = f(z) \\times (1 - f(z)) \\ ) $$\n",
    "  - Calculates the gradient with respect to the input.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL NEEDS TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------# \n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "class SigmoidActivation:\n",
    "    \"\"\"\n",
    "    Implements the Sigmoid activation function.\n",
    "    \n",
    "    The sigmoid function is defined as: f(z) = 1 / (1 + e^{-z})\n",
    "\n",
    "    \n",
    "    Attributes:\n",
    "        input_layer: The layer that provides the input to this activation function.\n",
    "        \n",
    "    Methods:\n",
    "        forward(input_array): Applies the Sigmoid activation function to the output of the input layer.\n",
    "        backward(downstream, input_array): Computes the gradient of the loss with respect to the input, which is then passed back to the previous layers.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(input_array):\n",
    "        \"\"\"\n",
    "        Compute the sigmoid activation for each element in the input_array.\n",
    "        \n",
    "        The sigmoid activation function is defined as:\n",
    "        f(z) = 1 / (1 + e^(-z))\n",
    "        \n",
    "        where:\n",
    "        - z is the input\n",
    "        - e is the base of natural logarithms (approximately equal to 2.71828)\n",
    "        \n",
    "        Parameters:\n",
    "        - input_array: Array of inputs to be passed through the sigmoid activation function.\n",
    "        \n",
    "        Returns:\n",
    "        - sigmoid: Array of outputs after applying the sigmoid activation.\n",
    "        \"\"\"\n",
    "        # Apply the Sigmoid activation function to the input array\n",
    "        # TODO 1. Compute the Exponential Term: For every element in the input array, you'll compute the negative exponential. \n",
    "        # In numpy, the function to compute the exponential of each element of an array is np.exp(). Using this function, \n",
    "        # compute the negative exponential of the input array.\n",
    "        negative_exponential = None\n",
    "        # TODO 2. Compute the Denominator: Add 1 to every element of the previously computed negative_exponential array.\n",
    "        denominator = None\n",
    "        # TODO 3. Divide 1 with the previously computed demonimator. \n",
    "        sigmoid = None\n",
    "        return sigmoid\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(downstream, input_array):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the loss with respect to the input of the sigmoid function.\n",
    "        \n",
    "        The derivative of the sigmoid function f(z) with respect to its input z is:\n",
    "        f'(z) = f(z) * (1 - f(z))\n",
    "\n",
    "        where:\n",
    "        f(z) is sigmoid\n",
    "\n",
    "        Parameters:\n",
    "        - downstream: The gradient of the loss with respect to the output of the sigmoid function.\n",
    "        - input_array: The original input to the sigmoid function.\n",
    "\n",
    "        Returns:\n",
    "        - input_grad: The gradient of the loss with respect to the input of the sigmoid function (the derivative of the sigmoid function).\n",
    "        \"\"\"\n",
    "        # Compute the gradient of the loss with respect to the input\n",
    "        # TODO Duplicate the sigmoid code from the forward function here\n",
    "        sigmoid = None\n",
    "        # TODO Compute the derivative of the sigmoid function \n",
    "        sigmoid_derivative = None\n",
    "        # TODO Compute the gradient with respect to the input by multiplying the gradient of the loss with respect to the output of sigmoid \n",
    "        # (downstream) by the derivative of the sigmoid (sigmoid_derivative) you computed above\n",
    "        input_grad = None\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `ReLUActivation` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "The `ReLUActivation` class implements the Rectified Linear Unit (ReLU) activation function. The ReLU function is defined as: \n",
    "$$ \\ f(z) = \\max(0,z) \\ $$\n",
    "\n",
    "### Attributes:\n",
    "- **input_layer**: The layer that provides input to this activation function.\n",
    "- **input_dimension**: The shape of the output from the `input_layer`.\n",
    "- **output_dimension**: The shape of the output of this layer. For ReLU, this is the same as `input_dimension`.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `forward(input_array: np.ndarray) -> np.ndarray`\n",
    "- **Purpose**: Applies the ReLU activation function to the output of the `input_layer`.\n",
    "- **Parameters**: \n",
    "  - `input_array`: Array of inputs to be passed through the ReLU activation function.\n",
    "- **Returns**: The output after applying the ReLU activation.\n",
    "- **Description**: \n",
    "  - Uses the numpy function `np.maximum()` to return the element-wise maximum values from two arrays.\n",
    "  - For the ReLU activation, this means \\( f(x) = \\max(0, x) \\) where \\( x \\) is `input_array`.\n",
    "\n",
    "---\n",
    "\n",
    "#### `backward(downstream: np.ndarray, input_array: np.ndarray = None) -> np.ndarray`\n",
    "- **Purpose**: Computes the gradient of the loss with respect to the input, and passes this back to the previous layers.\n",
    "- **Parameters**: \n",
    "  - `downstream`: The gradient of the loss with respect to the output of the ReLU function.\n",
    "  - `input_array`: The original input to the ReLU function. This is optional.\n",
    "- **Returns**: Gradient of the loss with respect to the input of the ReLU function.\n",
    "- **Description**: \n",
    "  - Computes the input gradient by multiplying the `downstream` gradient with an indicator array where values in the `input_array` greater than 0 are assigned a value of 1, and 0 otherwise.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL NEEDS TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------# \n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "\n",
    "class ReLUActivation:\n",
    "    \"\"\"\n",
    "    Implements the Rectified Linear Unit (ReLU) activation function.\n",
    "    \n",
    "    The relu function is defined as: f(z) = max(0,z)\n",
    "\n",
    "    Attributes:\n",
    "        input_layer: The layer that feeds input into this activation function.\n",
    "        input_dimension: The shape of the output from the input_layer.\n",
    "        output_dimension: The shape of the output of this layer, which \n",
    "        is the same as the input_dimension for ReLU.\n",
    "        \n",
    "    Methods:\n",
    "        forward(): Applies the ReLU activation function to the output \n",
    "        of the input layer.\n",
    "        backward(downstream): Computes the gradient of the loss with \n",
    "        respect to the input, to be passed back to the previous layers.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(input_array):\n",
    "        # TODO Use the numpy function np.maximum(). This function returns element-wise maximum values from two arrays.\n",
    "        # For the ReLU activation, the function is defined as f(x) = max(0, x) where x is your input_array\n",
    "        output_array = None\n",
    "        return output_array\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(downstream, input_array=None):\n",
    "        input_grad = downstream * (input_array > 0)\n",
    "        return input_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `BiasLayer` Class\n",
    "\n",
    "This class represents a `BiasLayer` in a neural network. The layer's main function is to add a bias term to the output of the preceding input layer. For every feature dimension of the input, a bias term is added.\n",
    "\n",
    "### Attributes:\n",
    "\n",
    "- **input_layer**: The preceding layer in the neural network. \n",
    "- **output_dimension**: The shape of the output from this layer, which is the same as the `input_layer`'s output dimension.\n",
    "- **W**: Bias matrix, initialized with random values, to represent bias terms added to the input.\n",
    "- **activation**: Activation function object (if provided). It can be either `SigmoidActivation`, `ReLUActivation`, or None.\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `__init__(self, input_layer, activation=None) -> None`\n",
    "\n",
    "- **Description**: Initializes the `BiasLayer`.\n",
    "- **Parameters**:\n",
    "  - `input_layer`: The preceding layer in the neural network.\n",
    "  - `activation`: The type of activation function to use, if any.\n",
    "- **TODOs**:\n",
    "  - Declare and initialize the bias matrix `self.W` with the shape `(1, num_input_features)`.\n",
    "  - Instantiate the appropriate activation function based on the provided `activation` parameter.\n",
    "\n",
    "---\n",
    "\n",
    "#### `forward() -> np.ndarray`\n",
    "\n",
    "- **Description**: Performs the forward pass, adding the bias terms and then applying the activation function (if defined).\n",
    "- **Returns**: The activated output if an activation is defined, otherwise the output after just adding the bias.\n",
    "- **TODOs**:\n",
    "  - Fetch the output of the preceding layer and store it in `self.input_array`.\n",
    "  - Add the bias term (`self.W`) to each feature dimension of `self.input_array`.\n",
    "  - If an activation function is defined, apply it to the `self.output_array`.\n",
    "\n",
    "---\n",
    "\n",
    "#### `backward(downstream: np.ndarray)`\n",
    "\n",
    "- **Description**: Computes the backward pass, propagating the gradient backwards through the activation (if defined) and then the bias addition.\n",
    "- **Parameters**:\n",
    "  - `downstream`: The gradient of the loss function with respect to the output of this layer.\n",
    "- **TODOs**:\n",
    "  - If an activation function is defined, compute the gradient of the loss with respect to the activated output, then propagate this gradient backward. If no activation is defined, propagate the downstream gradient directly backward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL NEEDS TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------# \n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "class BiasLayer:\n",
    "    \"\"\"\n",
    "    This layer adds a bias term to the output of the input layer. \n",
    "    Each feature dimension of the input gets a bias term.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_layer, activation=None) -> None:\n",
    "        self.input_layer = input_layer\n",
    "        num_data, num_input_features = input_layer.output_dimension\n",
    "        self.output_dimension = input_layer.output_dimension\n",
    "        # TODO: Declare the weight matrix (bias term) for the layer.\n",
    "        # Initialize a bias matrix `self.W` with the shape (1, num_input_features) using the `np.random.randn` function.\n",
    "        # This matrix will represent the bias terms added to the input.\n",
    "        self.W = None\n",
    "\n",
    "        if activation == 'Sigmoid':\n",
    "            self.activation = SigmoidActivation()\n",
    "\n",
    "        elif activation == 'ReLU':\n",
    "            self.activation = ReLUActivation()\n",
    "\n",
    "        else:\n",
    "            self.activation = None\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the bias layer.\n",
    "        \n",
    "        Returns:\n",
    "        The output array after adding the bias terms.\n",
    "        \"\"\"\n",
    "        # TODO: Fetch the output of the preceding layer and store it in `self.input_array`.\n",
    "        # You can achieve this by calling the `forward` method of `self.input_layer`.\n",
    "        self.input_array = None\n",
    "        \n",
    "        # TODO: Add the bias term to the fetched input.\n",
    "        # Use the bias matrix `self.W` to add the bias term to each feature dimension of `self.input_array` and store the result in `self.output_array`.\n",
    "        self.output_array = None\n",
    "        \n",
    "        # TODO: If an activation function is defined, apply it to the `self.output_array`.\n",
    "        # Call the `forward` method of the activation function and store the result in `self.activated_output`.\n",
    "        # Return the activated output. If no activation is defined, return the `self.output_array`.\n",
    "        if self.activation != None:\n",
    "            self.activated_output = None\n",
    "            return self.activated_output\n",
    "        \n",
    "        else:\n",
    "            return self.output_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def backward(self, downstream):\n",
    "        \"\"\"\n",
    "        Perform the backward pass.\n",
    "        \n",
    "        Parameters:\n",
    "        - downstream: The gradient of the loss function with respect to the output of this layer.\n",
    "        \"\"\"\n",
    "        self.G = downstream\n",
    "        \n",
    "        # TODO: If an activation function is defined, compute the gradient of the loss with respect to the activated output.\n",
    "        # Call the `backward` method of the activation function using the downstream gradient and the `self.activated_output` as arguments.\n",
    "        # Store the result in `activation_grad`.\n",
    "        # Then, compute the gradient of the loss with respect to the input of this layer by calling the `backward` method of `self.input_layer` with `activation_grad` as the argument.\n",
    "        # If no activation function is defined, directly call the `backward` method of `self.input_layer` using the downstream gradient as its argument.\n",
    "        if self.activation != None:\n",
    "            activation_grad = self.activation.backward(downstream, self.activated_output)\n",
    "            self.input_layer.backward(activation_grad)\n",
    "        else:\n",
    "            self.input_layer.backward(downstream)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions and Base Classes for Neural Network Implementations\n",
    "\n",
    "---\n",
    "\n",
    "### `is_modules_with_parameters` Function\n",
    "\n",
    "This function checks if the provided value is an instance of either `LinearLayer` or `BiasLayer`.\n",
    "\n",
    "#### Parameters:\n",
    "- `value` (object): The object to check.\n",
    "\n",
    "#### Returns:\n",
    "- `bool`: True if value is an instance of `LinearLayer` or `BiasLayer`, False otherwise.\n",
    "\n",
    "### `ModuleList` Class\n",
    "This class is an implementation of a mutable sequence to handle modules in a neural network.\n",
    "\n",
    "Methods:\n",
    "\n",
    "- `__getitem__`: Retrieve the i-th module.\n",
    "- `__setitem__`: Set the i-th module to v.\n",
    "- `__delitem__`: Delete the i-th module.\n",
    "- `__len__`: Return the number of modules.\n",
    "- `insert`: Insert module v at position i.\n",
    "- `get_modules_with_parameters`: Get modules that have parameters.\n",
    "\n",
    "### `BaseNetwork` Class\n",
    "This class serves as the base for neural network implementations.\n",
    "\n",
    "Methods:\n",
    "\n",
    "- `set_output_layer`: Set the output layer.\n",
    "- `get_output_layer`: Retrieve the output layer.\n",
    "- `__setattr__`: Overridden method to handle the setting of attributes, especially for modules with parameters.\n",
    "- `get_modules_with_parameters`: Get all modules that have parameters.\n",
    "- `forward`: Forward pass through the network.\n",
    "- `backward`: Backward pass through the network.\n",
    "- `state_dict`: Return the parameters of the modules.\n",
    "- `load_state_dict`: Load the parameters into the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ###### \n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "def is_modules_with_parameters(value):\n",
    "    \"\"\"\n",
    "    Checks if the provided value is an instance of either LinearLayer or BiasLayer.\n",
    "\n",
    "    Parameters:\n",
    "    - value (object): The object to check.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if value is an instance of LinearLayer or BiasLayer, False otherwise.\n",
    "    \"\"\"\n",
    "    return isinstance(value, LinearLayer) or isinstance(value, BiasLayer)\n",
    "\n",
    "\n",
    "class ModuleList(collections.abc.MutableSequence):\n",
    "    \"\"\"\n",
    "    An implementation of a mutable sequence to handle modules in a neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.list = list()\n",
    "        self.list.extend(list(args))\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"Retrieve the i-th module.\"\"\"\n",
    "        return self.list[i]\n",
    "\n",
    "    def __setitem__(self, i, v):\n",
    "        \"\"\"Set the i-th module to v.\"\"\"\n",
    "        self.list[i] = v\n",
    "\n",
    "    def __delitem__(self, i):\n",
    "        \"\"\"Delete the i-th module.\"\"\"\n",
    "        del self.list[i]\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of modules.\"\"\"\n",
    "        return len(self.list)\n",
    "\n",
    "    def insert(self, i, v):\n",
    "        \"\"\"Insert module v at position i.\"\"\"\n",
    "        self.list.insert(i, v)\n",
    "        pass\n",
    "    \n",
    "    def get_modules_with_parameters(self):\n",
    "        \"\"\"\n",
    "        Get modules that have parameters.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of modules with parameters.\n",
    "        \"\"\"\n",
    "        modules_with_parameters_list = []\n",
    "        for mod in self.list:\n",
    "            print(f\"Checking module: {mod}\")\n",
    "            if is_modules_with_parameters(mod):\n",
    "                print(f\"Adding module: {mod}\")\n",
    "                modules_with_parameters_list.append(mod)\n",
    "        print(f\"Final list of modules with parameters: {modules_with_parameters_list}\")\n",
    "        return modules_with_parameters_list\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class BaseNetwork:\n",
    "    \"\"\"\n",
    "    Base class for neural network implementations.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize BaseNetwork with an output layer and modules with parameters.\n",
    "        \"\"\"\n",
    "        super().__setattr__(\"initialized\", True)\n",
    "        super().__setattr__(\"modules_with_parameters\", [])\n",
    "        super().__setattr__(\"output_layer\", None)\n",
    "\n",
    "    def set_output_layer(self, layer):\n",
    "        \"\"\"Set the output layer.\"\"\"\n",
    "        super().__setattr__(\"output_layer\", layer)\n",
    "        pass\n",
    "\n",
    "    def get_output_layer(self):\n",
    "        \"\"\"Retrieve the output layer.\"\"\"\n",
    "        return self.output_layer\n",
    "\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        \"\"\"\n",
    "        Overridden method to handle the setting of attributes, especially for modules with parameters.\n",
    "        \"\"\"\n",
    "        print(f\"__setattr__ called with name: {name} and value: {value} type: {type(value)}\")\n",
    "\n",
    "        if not hasattr(self, \"initialized\") or (not self.initialized):\n",
    "            print(\"Initialization condition failed.\")\n",
    "            raise RuntimeError(\"You must call super().__init__() before assigning any layer in __init__().\")\n",
    "        print(\"Initialization condition passed.\")\n",
    "        if is_modules_with_parameters(value) or isinstance(value, ModuleList):\n",
    "            print(\"Module with parameters identified.\")\n",
    "            self.modules_with_parameters.append(value)\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "\n",
    "    def get_modules_with_parameters(self):\n",
    "        \"\"\"\n",
    "        Get all modules that have parameters.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of modules with parameters.\n",
    "        \"\"\"\n",
    "        modules_with_parameters_list = []\n",
    "        for mod in self.modules_with_parameters:\n",
    "            if isinstance(mod, ModuleList):\n",
    "                modules_with_parameters_list.extend(mod.get_modules_with_parameters())\n",
    "                pass\n",
    "            else:\n",
    "                modules_with_parameters_list.append(mod)\n",
    "                pass\n",
    "            pass\n",
    "        return modules_with_parameters_list\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        return self.output_layer.forward()\n",
    "\n",
    "    def backward(self, input_grad):\n",
    "        \"\"\"Backward pass through the network.\"\"\"\n",
    "        self.output_layer.backward(input_grad)\n",
    "        pass\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"\n",
    "        Return the parameters of the modules.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of parameters for all modules.\n",
    "        \"\"\"\n",
    "        all_params = []\n",
    "        for m in self.get_modules_with_parameters():\n",
    "            all_params.append(m.W)\n",
    "            pass\n",
    "        return all_params\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"\n",
    "        Load the parameters into the modules.\n",
    "\n",
    "        Parameters:\n",
    "        - state_dict (list): A list of parameters for the modules.\n",
    "        \"\"\"\n",
    "        assert len(state_dict) == len(self.get_modules_with_parameters())\n",
    "        for m, lw in zip(self.get_modules_with_parameters(), state_dict):\n",
    "            m.W = lw\n",
    "            pass\n",
    "        pass\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss Implementation\n",
    "\n",
    "---\n",
    "\n",
    "### Class: `CrossEntropyLoss`\n",
    "\n",
    "This class implements the binary cross-entropy loss function.\n",
    "\n",
    "The binary cross-entropy loss is defined as:\n",
    "$$ \\ \\text{loss} = - [y \\log(p) + (1 - y) \\log(1 - p)] \\ $$\n",
    "\n",
    "Where:\n",
    "- `y` is the true label (either 0 or 1).\n",
    "- `p` is the predicted probability of the data point being label 1.\n",
    "\n",
    "#### Attributes:\n",
    "- `input_layer`: The preceding layer of the neural network.\n",
    "- `labels`: Ground truth labels.\n",
    "\n",
    "#### Methods:\n",
    "- `set_data(labels)`: Set the ground truth labels.\n",
    "- `forward()`: Compute the forward pass, calculating the binary cross-entropy loss.\n",
    "- `backward()`: Compute the backward pass, calculating the gradient of the loss with respect to the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL NEEDS TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------# \n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    \"\"\"\n",
    "    Implements the binary cross-entropy loss function.\n",
    "\n",
    "    The binary cross-entropy loss is defined as: \n",
    "    \\( \\text{loss} = - [y \\log(p) + (1 - y) \\log(1 - p)] \\)\n",
    "    \n",
    "    Where:\n",
    "    - \\( y \\) is the true label (either 0 or 1).\n",
    "    - \\( p \\) is the predicted probability of the data point being label 1.\n",
    "\n",
    "    Attributes:\n",
    "        input_layer: The preceding layer of the neural network.\n",
    "        labels: Ground truth labels.\n",
    "        \n",
    "    Methods:\n",
    "        set_data(labels): Method to set the labels.\n",
    "        forward(): Computes the forward pass, calculating the cross-entropy loss.\n",
    "        backward(): Computes the backward pass, calculating the gradient of the loss with respect to the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dimension, labels=None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the CrossEntropyLoss class.\n",
    "        \"\"\"\n",
    "        self.input_layer = input_dimension\n",
    "        self.labels = labels\n",
    "    \n",
    "    def set_data(self, labels):\n",
    "        \"\"\"\n",
    "        Set the ground truth labels.\n",
    "\n",
    "        Parameters:\n",
    "        - labels (numpy.array): Ground truth labels.\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Computes the forward pass, calculating the binary cross-entropy loss.\n",
    "\n",
    "        TODOs:\n",
    "        1. Calculate the negative log likelihood for the true labels (y) using the formula.\n",
    "        2. Average the computed loss over all data points.\n",
    "\n",
    "        Returns:\n",
    "        - float: The average binary cross-entropy loss.\n",
    "        \"\"\"\n",
    "        # Fetch the predicted probabilities from the preceding layer's forward\n",
    "        self.p = self.input_layer.forward()\n",
    "        # batch size for samples\n",
    "        self.num_data = self.p.shape[1]\n",
    "\n",
    "        # TODO 1: Compute the loss using the provided formula for each data point\n",
    "        # - [y \\log(p) + (1 - y) \\log(1 - p)]\n",
    "        loss_values = None\n",
    "\n",
    "        # TODO 2: Average the computed loss over all data points. You can use np.mean for this.\n",
    "        self.out_array = None\n",
    "        return self.out_array\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Computes the backward pass, calculating the gradient of the loss with respect to the input.\n",
    "\n",
    "        The derivative of the binary cross-entropy loss with respect to the predicted probability \\( p \\) is:\n",
    "        \\( \\frac{\\partial \\text{loss}}{\\partial p} = \\frac{-y}{p} + \\frac{1 - y}{1 - p} \\)\n",
    "\n",
    "        TODOs:\n",
    "        1. Compute the derivative of the binary cross-entropy loss with respect to the predicted probabilities using the formula.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.array: The gradient of the loss with respect to the input.\n",
    "        \"\"\"\n",
    "        # TODO 1. Compute the negative gradient for true labels (-labels / predicted probabilities)\n",
    "        negative_gradient_for_positives = None\n",
    "        # TODO 2. Compute the gradient for false labels ((1 - labels) / (1 - predicted probabilities))\n",
    "        gradient_for_negatives = None\n",
    "        # TODO 3. Combine the gradients computed above by subtracting the gradient for true labels from the gradient for false labels and Normalize the gradient by the batch size\n",
    "        input_grad = None\n",
    "        \n",
    "        # Return the computed gradient to the preceding layers to continue the chain of gradients\n",
    "        self.input_layer.backward(input_grad)\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Class: `AdamSolver`\n",
    "\n",
    "This class implements the Adam optimization algorithm. Adam is a method for efficient stochastic optimization that only requires first-order gradients concerning the objective to make updates to the parameters.\n",
    "\n",
    "The Adam update rule is given by:\n",
    "$$ \\ W = W - \\text{lr} \\times \\frac{\\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon} \\ $$\n",
    "\n",
    "Where:\n",
    "- $$ \\ \\hat{m} = \\frac{m}{1 - \\beta1^t} \\ $$\n",
    "- $$ \\ \\hat{v} = \\frac{v}{1 - \\beta2^t} \\ $$\n",
    "\n",
    "#### Parameters:\n",
    "- **`lr` (float):** Learning rate\n",
    "- **`modules` (List[LinearLayer]):** List of layers in the model (excluding the input layer)\n",
    "- **`beta1` (float, optional):** Exponential decay rate for the first moment estimate. Default is 0.9.\n",
    "- **`beta2` (float, optional):** Exponential decay rate for the second moment estimate. Default is 0.999.\n",
    "- **`epsilon` (float, optional):** A tiny constant to prevent division by zero. Default is 1e-8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL NEEDS TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------# \n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "\n",
    "class AdamSolver:\n",
    "    \"\"\"\n",
    "    Implements the Adam optimization algorithm.\n",
    "    \n",
    "    Adam combines the benefits of both AdaGrad and RMSProp. It computes adaptive learning rates for each parameter. \n",
    "    In its equation, `m` and `v` are estimates of the first moment (the mean) and the second moment (the uncentered variance) \n",
    "    of the gradients respectively.\n",
    "\n",
    "    Update Rule:\n",
    "        m_t = beta1 * m_{t-1} + (1 - beta1) * g\n",
    "        v_t = beta2 * v_{t-1} + (1 - beta2) * g^2\n",
    "        m_hat = m_t / (1 - beta1^t)\n",
    "        v_hat = v_t / (1 - beta2^t)\n",
    "        w = w - lr * m_hat / (sqrt(v_hat) + epsilon)\n",
    "\n",
    "    Parameters:\n",
    "    - lr (float): Learning rate.\n",
    "    - modules (List[LinearLayer]): List of layers in the model (excluding the input layer).\n",
    "    - beta1 (float, optional): Exponential decay rate for first moment estimate. Default is 0.9.\n",
    "    - beta2 (float, optional): Exponential decay rate for second moment estimate. Default is 0.999.\n",
    "    - epsilon (float, optional): Small constant to prevent division by zero. Default is 1e-8.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate:float, modules: List[LinearLayer], beta1: float=0.9, beta2: float=0.999, epsilon: float=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.modules = modules\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "        for module in self.modules:\n",
    "            module.m = 0\n",
    "            module.v = 0\n",
    "       \n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Increment the time step `t`.\n",
    "        self.t += 1\n",
    "        for module in self.modules:\n",
    "            # g = 'module.G.mean(axis=0)' which computes the mean gradient across the batch.\n",
    "            # TODO: Update biased first raw moment estimate (beta1 * m_{t-1} + (1 - beta1) * g)\n",
    "            module.m = None\n",
    "            # TODO: Update biased second raw moment estimate (beta2 * v_{t-1} + (1 - beta2) * g^2)\n",
    "            module.v = None\n",
    "            # TODO: Compute bias-corrected first moment estimate (module.m / (1 - beta1^t))\n",
    "            m_m_hat = None\n",
    "            # TODO: Compute bias-corrected second moment estimate (module.v / (1 - beta2^t))\n",
    "            m_v_hat = None\n",
    "\n",
    "            module.W -= self.learning_rate * m_m_hat / (np.sqrt(m_v_hat) + self.epsilon)\n",
    "            pass\n",
    "        pass\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `SGDSolver` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `SGDSolver` class implements the Stochastic Gradient Descent (SGD) optimization algorithm for neural networks.\n",
    "\n",
    "### Algorithm Description:\n",
    "\n",
    "SGD updates each parameter \\( W \\) based on the gradient \\( G \\) of the objective function with respect to that parameter. The formula for the parameter update is:\n",
    "\n",
    "\\[ W = W - \\text{lr} \\times \\text{mean}(G) \\]\n",
    "\n",
    "where \\(\\text{lr}\\) is the learning rate and \\(\\text{mean}(G)\\) is the mean of the gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### Attributes:\n",
    "- **learning_rate (float)**: The learning rate for the optimization.\n",
    "- **modules (List)**: List of layers in the model, excluding the input layer. All layers should have a parent class of `LinearLayer`.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `__init__(self, learning_rate: float, modules: List[LinearLayer])`\n",
    "- **Purpose**: Initializes an instance of the `SGDSolver` class.\n",
    "- **Parameters**: \n",
    "  - **learning_rate (float)**: Learning rate for optimization.\n",
    "  - **modules (List)**: List of layers in the model, excluding the input layer.\n",
    "- **Description**: \n",
    "  1. Initializes the `learning_rate` attribute with the given `learning_rate`.\n",
    "  2. Initializes the `modules` attribute with the given `modules` list. This list should contain all the layers (excluding the input layer) of the neural network model.\n",
    "\n",
    "---\n",
    "\n",
    "#### `step(self)`\n",
    "- **Purpose**: Performs a single optimization step, updating the parameters of all layers in 'modules'.\n",
    "- **Description**: \n",
    "  - This method updates the parameters of all layers in the 'modules' list according to the SGD update rule.\n",
    "  - For each layer, it computes the mean gradient of its weights. This can be done using `np.mean(gradient of the layer, axis=0)` where `layer.G` represents the gradient of the layer.\n",
    "  - It then uses the computed mean gradient to update the layer's weights using the SGD formula.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL NEEDS TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------# \n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "\n",
    "class SGDSolver:\n",
    "    \"\"\"\n",
    "    Implements the Stochastic Gradient Descent (SGD) optimization algorithm.\n",
    "    \n",
    "    Algorithm Description:\n",
    "    SGD updates each parameter (W) based on the gradient (G) of the objective function \n",
    "    with respect to that parameter. The formula for the parameter update is:\n",
    "    \n",
    "    W = W - lr * mean(G)\n",
    "    \n",
    "    where lr is the learning rate and mean(G) is the mean of the gradients.\n",
    "    \n",
    "    Parameters:\n",
    "    - learning_rate (float): Learning rate.\n",
    "    - modules (List): List of layers in the model, excluding the input layer. \n",
    "                       All layers should have a parent class of LinearLayer.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate:float, modules:List[LinearLayer]):\n",
    "        # TODO 1: Initialize the `learning_rate` attribute with the given 'learning_rate'.\n",
    "        self.learning_rate = None\n",
    "        # TODO 2: Initialize the `modules` attribute with the given 'modules' list. This list should contain all the layers (excluding the input layer) of the neural network model.\n",
    "        self.modules = None\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Perform a single optimization step, updating the parameters of all layers in 'modules'.\n",
    "        \n",
    "        Description:\n",
    "        ------------\n",
    "        The method should update the parameters of all layers in the 'modules' list according\n",
    "        to the SGD update rule.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Loop through each module (layer) present in the `self.modules` list.\n",
    "        # The formula to update weights is: W = W - lr * mean(G)\n",
    "        # Replace the weights of the module (module.W) with the updated weights.\n",
    "        for module in self.modules:\n",
    "            # TODO 4: For each module, compute the mean gradient of its weights. This can be done using `np.mean(gradient of the module, axis=0)` where `module.G` represents the gradient of the module.\n",
    "            mean_gradient = None\n",
    "            # TODO 5: Use the computed mean gradient to update the module's weights using the SGD formula. Remember to scale the mean gradient with the learning rate `self.learning_rate` W = W - lr * mean gradient.\n",
    "            module.W = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `InputLayer` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `InputLayer` class represents the input layer of a neural network. The input layer essentially acts as a pass-through for the input data and does not perform any transformations.\n",
    "\n",
    "---\n",
    "\n",
    "### Attributes:\n",
    "- **W**: Identity matrix corresponding to the dimensions of the input data. This is included for compatibility with other layers but is not used.\n",
    "- **output_dimension**: The dimensions of the output (same as input in the case of `InputLayer`).\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `__init__(self, data_layer) -> None`\n",
    "- **Purpose**: Initializes an instance of the `InputLayer` class.\n",
    "- **Parameters**: \n",
    "  - **data_layer**: The layer that provides the input data to this layer.\n",
    "- **Description**: \n",
    "  - Initializes the output dimensions of this layer based on the provided `data_layer`.\n",
    "\n",
    "---\n",
    "\n",
    "#### `forward(self)`\n",
    "- **Purpose**: Passes the input data through the layer without altering it.\n",
    "- **Returns**: The input data without any modifications.\n",
    "- **Description**: \n",
    "  - Retrieves the input data from the associated `data_layer` and returns it as-is.\n",
    "\n",
    "---\n",
    "\n",
    "#### `backward(self, downstream)`\n",
    "- **Purpose**: Placeholder function for the backward pass.\n",
    "- **Parameters**: \n",
    "  - **downstream**: The gradient of the loss with respect to the output of the input layer.\n",
    "- **Description**: \n",
    "  - This function is a placeholder and does not perform any actual backward pass computations in the input layer.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ###### \n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "class InputLayer:\n",
    "    \"\"\"\n",
    "    Represents the input layer of a neural network. The input layer essentially acts as\n",
    "    a pass-through for the input data and does not perform any transformations.\n",
    "\n",
    "    Attributes:\n",
    "        W: Identity matrix corresponding to the dimensions of the input data. \n",
    "           This is included for compatibility with other layers but is not used.\n",
    "        output_dimension: The dimensions of the output (same as input in the case of InputLayer).\n",
    "\n",
    "    Methods:\n",
    "        forward(input_data): Passes the input data through the layer without altering it.\n",
    "        backward(downstream): Placeholder function; no actual backward pass computations \n",
    "                              are performed in the input layer.\n",
    "    \"\"\"\n",
    "     \n",
    "    def __init__(self, data_layer) -> None:\n",
    "        num_data, num_in_features = (data_layer.output_dimension)\n",
    "        self.output_dimension = np.array([num_data, num_in_features])\n",
    "        self.data_layer = data_layer\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.input_array = self.data_layer.forward()\n",
    "        return self.input_array\n",
    "\n",
    "    def backward(self, downstream):\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `OutputLayer` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `OutputLayer` class represents the output layer in a neural network. It inherits from the `LinearLayer` class and allows the use of activation functions on the output.\n",
    "\n",
    "---\n",
    "\n",
    "### Attributes:\n",
    "- **activation**: The activation function to be used after the linear operation.\n",
    "- **activated_output**: The output after applying the activation function.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `__init__(self, input_layer, num_out_features)`\n",
    "- **Purpose**: Initializes an instance of the `OutputLayer` class.\n",
    "- **Parameters**: \n",
    "  - **input_layer**: The layer that provides input to this output layer.\n",
    "  - **num_out_features**: Number of features in the output.\n",
    "- **Description**: \n",
    "  - Calls the constructor of the superclass (`LinearLayer`) to initialize the layer.\n",
    "\n",
    "---\n",
    "\n",
    "#### `forward(self)`\n",
    "- **Purpose**: Performs the forward pass by applying the linear operation and the activation function (if any).\n",
    "- **Returns**: The output after the linear operation (and activation, if specified).\n",
    "- **Description**: \n",
    "  - Calls the forward method of the superclass (`LinearLayer`) and returns the result.\n",
    "\n",
    "---\n",
    "\n",
    "#### `backward(self, downstream)`\n",
    "- **Purpose**: Computes the gradient of the loss with respect to the input of the output layer.\n",
    "- **Parameters**: \n",
    "  - **downstream**: The gradient of the loss with respect to the output of the output layer.\n",
    "- **Description**: \n",
    "  - Calls the backward method of the superclass (`LinearLayer`) to compute the gradients.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ###### \n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "\n",
    "class OutputLayer(LinearLayer):\n",
    "    \"\"\"\n",
    "    Represents the output layer in a neural network, inheriting from the Linear class.\n",
    "    It allows the use of activation functions on the output.\n",
    "    \n",
    "    Attributes:\n",
    "        activation: The activation function to be used after the linear operation.\n",
    "        activated_output: The output after applying the activation function.\n",
    "\n",
    "    Methods:\n",
    "        forward(): Performs the forward pass by applying the linear operation and activation function.\n",
    "        backward(dwnstrm): Performs the backward pass to compute the gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_layer, num_out_features):\n",
    "        super().__init__(input_layer, num_out_features)\n",
    "\n",
    "    def forward(self):\n",
    "        _out = super().forward()\n",
    "        return _out\n",
    "\n",
    "    def backward(self, downstream):\n",
    "        super().backward(downstream)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `Data` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `Data` class is designed to store an input array of training data and pass it to the next layer. It's worth noting that while this class and the `InputLayer` could essentially perform the same function, they have been separated solely for learning purposes.\n",
    "\n",
    "---\n",
    "\n",
    "### Attributes:\n",
    "- **data**: The stored input array of training data.\n",
    "- **output_dimension**: The shape or dimensions of the stored data.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `__init__(self, data)`\n",
    "- **Purpose**: Initializes an instance of the `Data` class.\n",
    "- **Parameters**: \n",
    "  - **data**: The input array of training data.\n",
    "- **Description**: \n",
    "  - Stores the provided data and initializes the `output_dimension` attribute based on the shape of the data.\n",
    "\n",
    "---\n",
    "\n",
    "#### `set_data(self, data)`\n",
    "- **Purpose**: Updates the stored data.\n",
    "- **Parameters**: \n",
    "  - **data**: New input array of training data to be stored.\n",
    "- **Description**: \n",
    "  - Replaces the current stored data with the provided data.\n",
    "\n",
    "---\n",
    "\n",
    "#### `forward(self)`\n",
    "- **Purpose**: Passes the stored data forward.\n",
    "- **Returns**: The stored data.\n",
    "- **Description**: \n",
    "  - Simply returns the stored data without any modifications.\n",
    "\n",
    "---\n",
    "\n",
    "#### `backward(self, dwnstrm)`\n",
    "- **Purpose**: Placeholder function for the backward pass.\n",
    "- **Parameters**: \n",
    "  - **dwnstrm**: The gradient of the loss with respect to the output of the data.\n",
    "- **Description**: \n",
    "  - This function is a placeholder and does not perform any operations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ###### \n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "\n",
    "class Data:\n",
    "    \"\"\"\n",
    "    Stores an input array of training data, and hands it to the next layer.\n",
    "    THIS AND THE INPUT LAYER COULD ESSENTIALLY PERFORM THE SAME FUCTION. \n",
    "    THEY HAVE BEEN SEPERATED SOLELY FOR LEARNING PURPOSES!!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.output_dimension = self.data.shape\n",
    "\n",
    "    def set_data(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def forward(self):\n",
    "        return self.data\n",
    "\n",
    "    def backward(self, dwnstrm):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `evaluate_model` Function Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `evaluate_model` function evaluates a model using various metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared, and potentially accuracy for classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **y_true (array-like)**: The ground truth target values.\n",
    "- **y_pred (array-like)**: The predicted values from the model.\n",
    "- **classification (bool, default: False)**: Specifies whether the task is classification. If `True`, accuracy will be computed; otherwise, only regression metrics will be provided.\n",
    "- **threshold (float, default: 0.5)**: The threshold for classifying outputs as positive for classification tasks. Outputs greater than this threshold will be classified as the positive class.\n",
    "\n",
    "---\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **dict**: A dictionary containing the computed metrics. This includes:\n",
    "  - `Mean Absolute Error`: The MAE between `y_true` and `y_pred`.\n",
    "  - `Mean Squared Error`: The MSE between `y_true` and `y_pred`.\n",
    "  - `R-squared`: The R-squared score between `y_true` and `y_pred`.\n",
    "  - `Accuracy` (optional): If `classification` is `True`, the accuracy of the predictions based on the provided threshold.\n",
    "\n",
    "---\n",
    "\n",
    "### Description:\n",
    "\n",
    "The function starts by computing the MAE, MSE, and R-squared between the true values (`y_true`) and the predicted values (`y_pred`). These values are stored in a dictionary named `metrics`.\n",
    "\n",
    "If the task is classification (i.e., `classification=True`), the function then converts probabilistic outputs to class labels based on the provided `threshold`. It computes the accuracy between the true labels and the predicted class labels and adds this accuracy value to the `metrics` dictionary.\n",
    "\n",
    "Finally, the function returns the `metrics` dictionary containing all computed values.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ###### \n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "def evaluate_model(y_true, y_pred, classification=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate the model using MAE, MSE, R-squared, and accuracy metrics.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (array-like): The ground truth target values.\n",
    "        y_pred (array-like): The predicted values from the model.\n",
    "        classification (bool): Whether the task is classification or not. Default is False.\n",
    "        threshold (float): Threshold to classify as positive class for classification tasks. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the MAE, MSE, R-squared, and possibly accuracy values.\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'Mean Absolute Error': mae,\n",
    "        'Mean Squared Error': mse,\n",
    "        'R-squared': r2,\n",
    "    }\n",
    "    \n",
    "    if classification:\n",
    "        # Convert probabilistic outputs to class labels based on threshold\n",
    "        y_pred_class = (y_pred > threshold).astype(int)\n",
    "        accuracy = accuracy_score(y_true, y_pred_class)\n",
    "        metrics['Accuracy'] = accuracy\n",
    "\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `plot_classification_predictions` Function Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `plot_classification_predictions` function visualizes classification predictions by plotting the true and predicted class labels for given feature data. Correctly classified samples are highlighted in blue with circles, while incorrectly classified samples are shown in red with crosses.\n",
    "\n",
    "---\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **x**: Feature data.\n",
    "- **y_true**: True class labels.\n",
    "- **y_pred**: Predicted class labels.\n",
    "\n",
    "---\n",
    "\n",
    "### Description:\n",
    "\n",
    "1. The function first identifies which samples are correctly and incorrectly classified by comparing `y_true` and `y_pred`.\n",
    "\n",
    "2. Using matplotlib's `scatter` function:\n",
    "    - Correctly classified samples are plotted in blue with circles (`marker='o'`).\n",
    "    - Incorrectly classified samples are plotted in red with crosses (`marker='x'`).\n",
    "\n",
    "3. The plot is then decorated with appropriate labels, a legend, and a grid. The x-axis represents the feature values, while the y-axis represents the class (either true or predicted). \n",
    "\n",
    "4. Finally, the plot is displayed using `plt.show()`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ###### \n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "def plot_classification_predictions(x, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Plot classification predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - x: Feature data\n",
    "    - y_true: True class labels\n",
    "    - y_pred: Predicted class labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify correctly and incorrectly classified samples\n",
    "    correct = y_true == y_pred\n",
    "    incorrect = np.logical_not(correct)\n",
    "\n",
    "    # Plot correctly classified samples\n",
    "    plt.scatter(x[correct], y_true[correct], color='blue', label='Correct', marker='o', alpha=0.5)\n",
    "\n",
    "    # Plot incorrectly classified samples\n",
    "    plt.scatter(x[incorrect], y_true[incorrect], color='red', label='Incorrect', marker='x', alpha=0.5)\n",
    "\n",
    "    plt.xlabel('Feature Value')\n",
    "    plt.ylabel('Class')\n",
    "    plt.title('Classification Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "This section provides the markdown documentation for the `Network` and `Trainer` classes, focusing on their structure, attributes, and methods. These classes are fundamental for defining and training a neural network.\n",
    "\n",
    "---\n",
    "\n",
    "## `Network` Class\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `Network` class inherits from the `BaseNetwork` and is responsible for defining the architecture of the neural network, including the input, hidden, and output layers.\n",
    "\n",
    "---\n",
    "\n",
    "### Attributes:\n",
    "\n",
    "- **MY_MODULE_LIST**: A module list that will store the various layers of the network.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `__init__(self, data_layer, hidden_layers, hidden_units)`\n",
    "- **Purpose**: Initializes an instance of the `Network` class.\n",
    "- **Description**: \n",
    "  - Initializes the base network.\n",
    "  - Constructs the neural network layers based on the provided parameters.\n",
    "  \n",
    "---\n",
    "\n",
    "## `Trainer` Class\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `Trainer` class is responsible for defining the network, setting it up, and performing training operations.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `define_net(self, data_layer, parameters=None)`\n",
    "- **Purpose**: Defines the network architecture.\n",
    "- **Description**: \n",
    "  - Sets the hidden units and layers based on the provided parameters.\n",
    "  - Returns the defined network.\n",
    "  \n",
    "---\n",
    "\n",
    "#### `net_setup(self, train_data)`\n",
    "- **Purpose**: Sets up the network for training.\n",
    "- **Description**: \n",
    "  - Defines the data layer based on the provided training data.\n",
    "  - Defines the network, loss layer, and optimizer.\n",
    "  - Returns the data layer, network, loss layer, and optimizer.\n",
    "\n",
    "---\n",
    "\n",
    "#### `train_step(self)`\n",
    "- **Purpose**: Performs a single training step.\n",
    "- **Description**: \n",
    "  - Computes the loss, performs backward propagation, and updates the network parameters.\n",
    "  - Returns the computed loss.\n",
    "\n",
    "---\n",
    "\n",
    "#### `train(self, number_of_iterations)`\n",
    "- **Purpose**: Trains the network for a specified number of iterations.\n",
    "- **Description**: \n",
    "  - Iteratively performs training steps and stores the loss for each step.\n",
    "  - Returns a list of training losses.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL NEEDS TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------# \n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "\n",
    "# MMEEEEEE: WORK ON TODOS\n",
    "\n",
    "# TODO Experiment to find the munber of iterations\n",
    "Number_of_iterations = 0\n",
    "# TODO Experiment to find the right step_size/learning_rate for the network\n",
    "learning_rate = 0.00\n",
    "\n",
    "class Network(BaseNetwork):\n",
    "    def __init__(self, data_layer, hidden_layers, hidden_units):\n",
    "        super().__init__()\n",
    "        self.MY_MODULE_LIST = ModuleList()\n",
    "        # TODO Append the input layer to the MODULE LIST\n",
    "        self.MY_MODULE_LIST.append(None)\n",
    "        for _ in range(hidden_layers):\n",
    "            # TODO Since each hidden layer is accompanied to a bias, use the for loop to add \n",
    "            # the number of hidden layers passed in and their corresponding bias layers. \n",
    "            # Don't forget to add an activation fuction for your biases. \n",
    "            # You can find the last layer in the list by using the list index or the __getitem__ \n",
    "            # and passing it the index of the length of the list minus 1 (len(self.MY_MODULE_LIST) - 1)\n",
    "            self.MY_MODULE_LIST.append(HiddenLayer(input_dimension=None, output_dimension=random.choice(hidden_units)))\n",
    "            self.MY_MODULE_LIST.append(BiasLayer(input_layer=None, activation=None))\n",
    "\n",
    "        # TODO Now add the output layer to the MODULE LIST. As this is binary classification, \n",
    "        # it is needed that we add a bias and activation to the output layer. Pick an activation \n",
    "        # function that will ensure that the prediction of the model is squashed between 0 and 1.\n",
    "        self.MY_MODULE_LIST.append(OutputLayer(input_layer=None, num_out_features=1))\n",
    "        self.MY_MODULE_LIST.append(BiasLayer(input_layer=None, activation=None))\n",
    "        # TODO set the output layer to the last layer in self.MY_MODULE_LIST.\n",
    "        self.set_output_layer(None)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def define_net(self, data_layer, parameters=None):\n",
    "        # TODO this network is a deep network and hense, has multiple hidden layers and multiple hidden units\n",
    "        # set the hidden units and layers to the values passed in parameters\n",
    "        hidden_units = None\n",
    "        hidden_layers = None\n",
    "        network = Network(data_layer, hidden_layers=hidden_layers, hidden_units=hidden_units)\n",
    "        print(network)\n",
    "        return network\n",
    "    \n",
    "    def net_setup(self, train_data):\n",
    "        features, labels = train_data\n",
    "        self.data_layer = Data(features)\n",
    "        # TODO pass in an Integer for hidden_layers and a list of integers to hidden_units\n",
    "        self.network = self.define_net(self.data_layer, parameters={'hidden_layers': 0, 'hidden_units': [0]})\n",
    "        # TODO Select the approraiate loss function and pass it the required parameters\n",
    "        self.loss_layer = None\n",
    "        # TODO Select the approraiate optimizer and pass it the required parameters\n",
    "        self.optimizer = None\n",
    "        return self.data_layer, self.network, self.loss_layer, self.optimizer\n",
    "    \n",
    "    def train_step(self):\n",
    "        loss = self.loss_layer.forward()\n",
    "        self.loss_layer.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "    \n",
    "    def train(self, number_of_iterations):\n",
    "        train_losses = []\n",
    "        for _ in tqdm(range(number_of_iterations), desc=\"Training\", leave=True):\n",
    "            train_losses.append(self.train_step())\n",
    "        return train_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `main` Function Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `main` function is designed to set up a training environment using the `Trainer` class. It orchestrates the process of data loading, network setup, training, and evaluation. It also plots training losses over iterations and displays evaluation metrics. If in test mode, it returns specific objects necessary for automated testing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ###### \n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "def main(test=False):\n",
    "    # setup the trainer\n",
    "    trainer = Trainer()\n",
    "\n",
    "    # DO NOT REMOVE THESE IF/ELSE\n",
    "    if not test:\n",
    "        data = q3_a_nl()\n",
    "        data_layer, network, loss_layer, optimizer = trainer.net_setup(data['train'])\n",
    "        loss = trainer.train(Number_of_iterations)\n",
    "        plt.plot(loss)\n",
    "        plt.ylabel('Loss of NN')\n",
    "        plt.xlabel('Number of Iterations')\n",
    "        plt.show()\n",
    "\n",
    "        # Now let's use the test data\n",
    "        x_test, y_test = data['test']\n",
    "        test_data_layer = Data(x_test)\n",
    "        input_layer = InputLayer(test_data_layer)\n",
    "        trainer.data_layer.set_data(input_layer)\n",
    "        trainer.network.MY_MODULE_LIST[1].input_layer = input_layer\n",
    "        \n",
    "        # Get predictions for test data\n",
    "        y_pred = network.MY_MODULE_LIST[-1].forward()\n",
    "\n",
    "        metrics = evaluate_model(y_test, y_pred, classification=True)\n",
    "        # Print the metrics for review\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "        # Plot actual vs predicted on test data\n",
    "        y_pred = (y_pred > 0.5).astype(int)\n",
    "        \n",
    "        plot_classification_predictions(x_test, y_test, y_pred)\n",
    "\n",
    "    else:\n",
    "        # DO NOT CHANGE THIS BRANCH! This branch is used for autograder.\n",
    "        out = {\n",
    "            'trainer': trainer\n",
    "        }\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ###### \n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
